\begin{comment}
2.3 Teil II: SW-Projektdokumentation
Hier folgen die Dokumente gemäss Software-Engineering-Vorgehen.
2.3.1 Überblick
Zweck und Inhalt dieses Kapitels
2.3.2 Vision
Verweis auf Teil I.
2.3.3 Anforderungsspezifikation
Enthält folgende mögliche Unterkapitel:
 Rahmenbedingungen (wenn nicht schon oben abgehandelt)
 Anwendungsfalldiagramm
 Hauptanwendungsfall
 Funktionale Anforderungen
 Nicht-Funktionale Anforderungen
 Weitere: Aktivitätsdiagramme, Fallbeispiele, Szenarien, Prototypen...
2.3.4 Analyse
Klassen-, bzw. Domainmodell.
2.3.5 Design (Entwurf)
Die Architektur soll eine objektorientierte Problemdomain umfassen. Eine allfällig eingesetzte Datenbank darf diese Problemdomain permanent speichern, nicht aber ersetzen.
Enthält folgende mögliche Unterkapitel:
 Klassenverantwortlichkeiten (Klassenname, Verantwortlichkeit, Wissen, Tun, Abhängigkeiten); CRC- Diagramme?
 Sequenzdiagramm / Kollaborationsdiagramm
2.3.6 Implementation (Entwicklung)
Objektkatalog: könnte mit einem Thesaurus verwaltet werden! Rest individuell.
2.3.7 Test
Enthält folgende mögliche Unterkapitel:
 Testverfahren automatisch (mit JUnit) und manuell (dokumentiert; z.B. GUI); durchnummeriert.
 Iterationen 1 ... X
2.3.8 Resultate
Resultate und Ergebnisse der Arbeit. Dieser Abschnitt richtet sich an den speziell für das entsprechende Fachgebiet interessierten Ingenieur. Er soll es ihm ermöglichen, die für die Problemlösung gemachten Überlegungen zu verstehen und nachzuvollziehen.
2.3.9 Weiterentwicklung
Möglichkeiten der Weiterentwicklung: Funktionen und mögliches weiteres Vorgehen. Weiterentwicklung scheint allgemein ein heikler Punkt zu sein in allen Projekten, besonders auch in SA/DA-Projekten:
In realen Projekten, im RUP-Prozess wird er kaum betont und auch für Sie und für die Auftraggeber scheint es ein Problem zu sein. Warum?
 Wenn man dort zuviel aufschreibt, dann könnte das als Manko aufgefasst werden; und Aufschreiben nimmt erst noch Zeit weg...
 Zudem ist aus Sicht Auftraggeber (ohne "Gegensteuer") eine zusätzliche Funktion besser bewertbar als der "Wert" eines robusten, sauberen Designs oder eines Refactorings (z.B. der Separierung der Objekt/Klassen-Zuständigkeiten).
Ein Test, wie "sauber" - namentlich: wie änderungsfreundlich - ein Design ist, zeigt sich beispielsweise in der Antwort auf folgende Frage: "Welche Interfaces muss man implementieren und/oder welche Komponenten/Klassen muss man erweitern (schlimmstenfalls anpassen), um eine weitere Funktionalität einzubauen? Z.B. ein weiterer Exportfilter?".
Daher gelten folgende Regelungen zur Weiterentwicklung (gilt auch für andere Projektarten):
 Weiterentwicklung ist obligatorisch und erscheint in zwei separaten Kapitel (ggf. Dokument): Im Technischen Bericht, Unterkapitel Resultate/Ausblick, und in der SW-Projektdokumentation.
 Weiterentwicklung im Technischen Bericht ist allgemein gehalten und daher weniger heikel. Wichtig ist die Aufzählung der hauptsächlichen weiteren möglichen funktionalen oder nicht-funktionalen Anforderungen.
 Weiterentwicklung in der SW-Projektdokumentation ist an Architekten / SW-Entwickler gerichtet wie jedes andere SW-Dokument.
 Gewichtung: Es wird separat bewertet und zusätzlich erst spät (SA/DA bei der Präsentation) vollständig abgegeben wird. Spät heisst bei DAs auf der Dokumentation der Präsentation (inkl. CD). Sein ungewichtetes Gewicht gegenüber der Gesamtdokumentation ist umfangmässig ca. 1/15. Zur Erinnerung siehe Unterkapitel Lieferdokumente mit ca. 15 Hauptkapitel-/-Dokumenten.
2.3.10 Benutzerdokumentation
Installationsanleitung(en), Bedienungsanleitung(en) und Tutorien (evtl. in den Anhang)! Vergessen Sie a) nicht den CD-Inhalt zu notieren und auch in die Doku. zu nehmen.
Testen sie die Installation mit realistischen Vorgaben!!
2.4.1 Allgemeines
 Normen
 Konfigurationsmanagement (Entwicklungs-Werkzeuge, Eingesetzte Software)
2.4.2 Projektmanagement
Enthält folgende mögliche Unterkapitel:
 Vorgehen
 Zeitplanung
 (Erreichen der Ziele siehe sep. Kapitel "Bewertung und Ausblick").
Zeitplanung: Die Zeitplanung orientiert sich an den Meilensteinen und ist nach folgender Idee strukturiert:
     Inception => Elaboration => Construction 1 => Construction 2 => ... => Transition
Das ergibt folgendes Zeitdiagramm:
￼2.4.3
                          Dokumente
Dokumente Teil I:             |
+ Einführung                  |
+ Vision                      |
+ "Stand der Technik"         |
+ Umsetzungskonzept           |
+ Resultate                   |
Dokumente Teil II:            |
+ Anforderungsspezifikation   |
+ Analyse                     |
+ Design                      |
+ Implementation              |
+ Projektmgmt.&-Monitoring    |
+ Test                        |
+ Resultate und Weiterentw.   |
+ Benutzerdokumentation       |
+ Glossar und Abkürzungsverz. |
+ Literatur- und Quellenverz. |
                              +----+------+---------+---------+------> Tage
               Zeitabschnitte: Inc.  Elab.  Constr.1  Constr.2  Trans.
Projektmonitoring
Code-Analyse (Metriken).
\end{comment}

\part{Projektdokumentation}

\subimport{}{anforderungen.tex}

\chapter{Analyse}


\section{Dateiformate}

In diesem Abschnitt werden die durch den Kunden gewünschten oder zur Realisierung der Anwendungsfälle in \cref{sec:pd:usecases} benötigten Dateiformate und deren Eigenheiten beschrieben.

\subsection{CSV}
\acf{csv} gilt trotz vielen Schwächen als universellen Austausch/Exportformat für einfache Anwendungsfälle und sollte deshalb zwingend unterstützt werden. Eine \acs{csv} Datei enthält einen Datensatz pro Zeile und die Spalten üblicherweise durch Kommas oder Semikolon getrennt. Die erste Zeile enthält die Namen der einzelnen Felder.

\subsubsection{Erweiterung mit Typen und Geo-Daten}

\gls{geocsv} ist eine durch das Geometa-Lab der HSR spezifizierte Erweiterung der CSV Spezifikation um Geo-Daten mit folgenden Eigenheiten\cite{sfkeller,geocsv,gdal-csv}:

\begin{itemize}
\item Eine optionale Datei mit Endung \path{.csvt} spezifiziert auf einer einzigen Zeile die Datentypen genauer
\item Der spezielle Datentyp ``WKT'' für Geometrien im \gls{wkt} Format
\item Die speziellen Datentypen ``Point(x)'', ``Point(y)'' oder ``CoordX'' bzw. ``CoordY'' für den Spezialfall einer Punktgeometrie
\item Eine optionale Datei mit der Endung \path{.prj} zur Angabe des Referenzsystems, ebenfalls im \gls{wkt} Format\footnote{Achtung, das \path{.prf} Format von ESRI sieht sehr ähnlich aus, enthält jedoch weniger Informationen!}
\item Nur ein Geometrie-Objekt/Spalte pro Datei
\end{itemize}


\subsection{Excel}
Da viele nicht-technische Benutzer mit Microsoft Excel arbeiten\cite{sfkeller} oder Daten im Excel Format anliefern, müssen sowohl der alte Excel 2003 (xls) sowie der neue OpenXML (xlsx) Formate unterstützt werden.

\subsection{GeoJSON}
\gls{geojson} ist eine auf JSON basierende Schema-Spezifikation für Geo-Daten. Dieses Format ist heutzutage weit verbreitet und fast überall wo mit Geo-Daten hantiert wird unterstützt.

\subsection{GeoPackage}
GeoPackage ist ein relativ neues, auf SQLite basiertes, Format. Es ist sehr nahe an SpatiaLite, welches mit Version 4.2 auf GeoPackage wechselt.

\subsection{GML}
\gls{gml} ist ein auf XML basierendes Format für Geo-Daten und ist das Standard-Ausgabeformat von \acs{wfs}-Servern.


\subsection{INTERLIS}
Interlis besteht aus einer Modellbeschreibungssprache und einem Transferformat. Da es praktisch nur in der Schweiz zum Einsatz kommt ist die Unterstützung in kommerzieller Software mässig bis nicht vorhanden.

Interlis existiert in zwei Versionen: Interlis 1 verwendet ein Record-basiertes Transfer-Format, während für Interlis 2 XML gewählt wurde. Mit wenigen Ausnahmen stellt Interlis 2 eine Erweiterung von Interlis 1 um Objekt-orientierte Features dar.

\subsection{KML}
\gls{kml} ist wie \gls{gml} ebenfalls ein auf \gls{xml} basierendes Format, welches in Google Earth Verwendung findet. Speziell bei KML, ist dass es lediglich das Referenzsystem 4326 (klassisches lat/lon) unterstützt.


\subsection{ESRI Shapefile}
Das Shapefile ist ein von der Firma ESRI spezifiziertes Formt zum Austausch von Geo-Daten. Es wird von fast allen Bibliotheken und Systemen unterstützt und eignet sich somit als universelles Austauschformat für Geo-Daten. Das Shapefile wird immer wieder aufgrund dessen Schwächen kritisiert. Da wären beispielsweise die Limitation der Spaltennamen auf 8 Zeichen sowie von Textspalten auf 255 Zeichen. Aus diesem Grund versuchen viele das veraltete Shapefile durch ein moderneres Format wie GeoJSON oder GeoPackage abzulösen\cite{sfkeller}

\subsection{WFS}
Bei \gls{wfs} handelt es sich um einen speziellen Webservice für Geo-Daten. Die Daten können üblicherweise im \gls{gml} Format bezogen werden. Da es sich dabei um einen weit verbreiteten Standard von \gls{ogc} handelt, sollte die Applikation einen direkten Bezug von Daten via einem \acs{wfs} Server ermöglichen.

\subsection{XML}
Da es sich bei \gls{xml} um eine potenziell verschachtelte Datenstruktur handelt, wird vorerst nur eine flache Version ähnlich der von der API von \purl{http://truckinfo.ch} gelieferten in \cref{sec:pd:usecases} beschriebenen Daten. 

\subsection{JSON}
Auch bei JSON handelt es sich um ein Format mit Schachtelungsmöglichkeit. Aus diesem Grund wird auch hier vorerst nur die oberste Ebene betrachtet.


\section{Datenspeicherung}
Ein Ziel dieser Arbeit ist die Erstellung eines Daten-Hubs, welcher es Anbietern ermöglicht, ihre Daten anderen zur Verfügung zu stellen. 

Zur Speicherung der Daten stehen folgende Varianten zur Auswahl.

\subsection{File-basiert}
Die Daten werden direkt im Dateisystem gespeichert. Um Konflikte zwischen Datei-Namen zu vermeiden muss ein Hashing-Schema definiert und verwendet werden. Metadaten zu den Dokumenten werden in einer Datenbank hinterlegt.

Die Daten werden in ihrem Ursprungsformat belassen, bis ein Nutzer eine Transformation auswählt und anwendet. Die Resultate einer solchen Transformation können direkt wieder im Dateisystem zwischengespeichert werden um weitere Abfragen nach der selben Transformation zu beschleunigen.

Die Tatsache, dass die Daten im Dateisystem liegen vereinfacht die Einbindung von Software von Drittanbietern, insbesondere wenn diese Software nur direkt auf Datei-Basis arbeitet (z.B. OGR).

Ein Nachteil dieser Variante ist, dass Cloud-Anbieter wie Heroku keine schreibbaren\cite{herokuReadOnlyFilesystem} bzw. keine beständigen\cite{herokuEphemeralFilesystem} Dateisysteme anbieten, sondern auf die Verwendung von Amazon S3 oder ähnlich verweisen.

Metadaten wie Format oder Original-Dateinamen würden aus Performance-Gründen weiterhin in einer Datenbank gehalten werden.

\subsection{Datenbank-basiert, einzelne Records}\label{sec:pd:datenbank-records}
Die Daten werden in einzelne Records zerlegt und zusammen mit den Metadaten in einer Datenbank gespeichert. Dies erfordert, dass bereits der Daten-Anbieter eine Transformation in das Record-Format auswählen muss. Falls sich herausstellt, dass dies zu langsam ist um es direkt im HTTP-Request des Anbieters durchzuführen, müssen die Daten vor der Transformation zwischengespeichert werden.

Falls die Transformationsresultate zwischengespeichert werden sollen ist eine weitere Datenablage für die Transformationsresultate notwendig.

Die Einbindung von Drittanbieter-Software erfordert mehr Aufwand, falls diese nur auf Dokumenten- oder gar Datei-Ebene arbeiten. In diesem Fall müssen die Daten erst wieder in ein geeignetes Format übersetzt werden, um nach der Transformation wieder in einzelne Records zerlegt zu werden.

\subsection{Datenbank-basiert, Dokument als BLOB}
Die Daten werden ohne Transformation zusammen mit den Metadaten in einer Datenbank abgelegt. Diese Variante hat praktisch alle Vorteile der File-basierten Version.

Falls Dritt-Anbieter-Software nur auf Datei-basis arbeitet kann ein temporäres oder flüchtiges Datei-System verwendet werden.

Für diese Variante existieren folgende Möglichkeiten:
\begin{description}
\item[Daten und Metadaten in der selben Tabelle] Daten und Metadaten werden in der selben Tabelle gespeichert. Dies bietet ein einfacheres Schema, erlaubt jedoch nur ein Daten-Objekt pro Dokument, was ungeeignet ist für Dokument-Typen wie ESRI Shapefiles, welche aus mehr als einer Datei bestehen.
\item[Daten und Metadaten in separaten Tabellen] Daten und Metadaten werden in unterschiedlichen Tabellen gespeichert. Dies erlaubt die Speicherung von Dokumenten bestehend aus mehreren Dateien, oder transformierten Daten als zusätzlichen Daten-Eintrag zu einem Metadaten-Record. Dies erlaubt es auch, Django-Models zu erstellen welche nicht jedes mal die Daten abfragen, obwohl nur Metadaten benötigt werden.
\end{description}

Da File-Formate existieren, welche aus mehr als einer Datei bestehen, ist die Option mit separaten Tabellen vorzuziehen.

\subsubsection{Fazit}
\cref{tab:pd:filestorage-criteria} erläutert die Kriterien für die Bewertung der verschiedenen Varianten.
\mytable{lXc}{
  \textbf{Kriterium} & \textbf{Beschreibung} & \textbf{Gewichtung} \\
  \midrule
  \textbf{Original bleibt erhalten} & Das Original-Dokument bleibt erhalten oder kann komplett rekonstruiert werden. Dies bietet folgende Vorteile: Verbesserungen an der Software (Parser) zeigt auch bei alten Daten Wirkung, ausserdem gehen keine Daten verloren durch den Upload. & \threeOfThree \\
  \textbf{Einbindung Software} & Erlaubt die Einbindung von Drittanbieter-Software. Dies ist möglich auf File-basis, oder direkt als Memory Buffer, falls Python-Code aufgerufen wird. & \oneOfThree \\
  \textbf{Beliebige Formate} & Die Variante unterstützt beliebige Formate. Anschliessende Transformationen können nur auf bekannte Formate angewandt werden, aber auch unbekannte Formate können gespeichert und zum Download angeboten werden. Die Daten können verwendet werden, falls später ein Parser geschrieben wird für das bisher unbekannte Format. & \twoOfThree \\
  \textbf{Cloud} & Cloud-Anbieter wie Heroku bieten oft kein permanentes File-System, sondern verweisen auf Storage Provider wie Amazon S3 oder auf Datenbanken als Daten-Speicher. Unterstützung für Cloud-Anbieter ist kein zwingendes Kriterium für diese Arbeit, soll aber falls möglich erhalten bleiben. & \oneOfThree \\
}{Bewertungskriterien Datenspeicherung}{pd:filestorage-criteria}

\cref{tab:pd:transformation-lang-results} fasst die verschiedenen Optionen zusammen.
\mytable{lXXX}{
  \textbf{Kriterium} & \textbf{File} & \textbf{DB: Record} & \textbf{DB: Schhemalos} \\
  \midrule
  \textbf{Original bleibt erhalten} & \threeOfThree & \oneOfThree & \threeOfThree \\
  \textbf{Einbindung Software} & \threeOfThree & \twoOfThree & \threeOfThree \\
  \textbf{Beliebige Formate} & \threeOfThree & \oneOfThree & \threeOfThree \\
  \textbf{Cloud} & \twoOfThree & \threeOfThree & \threeOfThree \\
  \midrule
  \textbf{Total} & \threeOfThree & \oneOfThree & \threeOfThree \\
}{Auswertung verschiedener Speicher-Varianten}{pd:filestorage-results}

\begin{decision}[label=dec:pd:datenspeicherung]{Datenspeicherung}
Aus Gründen der Kompatibilität zu Cloud-Anbietern, insbesondere Heroku ohne gleich Cloud-Datenspeicher wie Amazon S3 verwenden zu müssen, wurde bereits früh entscheiden, die Dateien in der Datenbank zu persistieren. Da die Datenhaltung ohnehin eine sehr lose Kopplung zur restlichen Applikation besitzen soll, kann künftig auch eine andere Strategie implementiert werden.
\end{decision}

\section{Intermediäres Format}

Bei $N$ Dateiformaten würden bei 1-zu-1 Format-Konversionen bereits $\frac{N*(N-1)}2$ Übersetzungsmodule benötigt, um lediglich zwischen den Dateiformaten zu konvertieren. Zudem muss es möglich sein das Schema der Daten zu transformieren. Diese Tatsache erfordert zwingend ein intermediäres, einheitliches Format ohne welches eine generische Transformation unmöglich wäre.

In diesem Abschnitt werden diverse Optionen im Hinblick auf deren Vor- und Nachteile sowie Eignung beschrieben. 

\subsection{Datenbank}
Bei dieser Variante wäre das Intermediäre Format äquivalent der Datenspeicherung wie sie in \cref{sec:pd:datenbank-records} beschrieben ist. Die Daten würden aber dennoch separat als Originaldateien abgelegt werden um keine Informationen zu vernichten. Der grosse Vorteil dieser Variante ist, dass die Transformationen dann mittels SQL Abfragen auf die diversen Datenbanktabellen erfolgen kann. Man ist dabei jedoch auf die Funktionalität der Datenbank limitiert und kann nur sehr schlecht Transformationen anbieten die dessen Fähigkeiten übersteigen.

\xxx[Braucht noch Anmerkungen zu Performance, weil unten nur 1 von 3 Sternen vergeben wurde. Verluste in in Konversion Python -> SQL/SQL -> Python sind evtl. nicht offensichtlich - dies wird erst in der nächsten Section erwähnt.]

\subsection{ogrtools}

Ogrtools ist eine Python-Implementation des \acs{cli}-Tools ``ogr2ogr'' erweitert mit simplen Transformationen. Ogr2ogr wie auch ogrtools verwenden die kompilierten OGR/GDAL Bibliotheken, welche 1-zu-1 (ogr $\to$ ogr) Format-Transformationen ermöglichen. Beispielsweise können ESRI Shapefiles (.shp) in \gls{kml} oder \gls{gml} Dateien umgewandelt werden. Da die Formate untereinander nicht immer kompatibel sind kann es durchaus zum Verlust von Informationen kommen.

Sollte die ogrtools Bibliothek verwendet werden, so würde auf der existierenden Transformationssprache bzw. Tranformationskonfiguration aufgebaut und dieses um weitere Operationen wie beispielsweise Filterung oder Joins erweitert. Eine ogrtools Beispielskonfiguration \cite[README, ogrtransform library]{ogrtools} ist in \cref{src:pd:ogrtools} zu sehen.


\begin{srclst}[label=src:pd:ogrtools]{json}{ogrtools Mapping-Konfiguration}
{
  "//": "OGR transformation configuration",
  "src_format": "Interlis 2",
  "dst_format": "PostgreSQL",
  "dst_dsco": {},
  "dst_lco": {
    "SCHEMA": "public"
  },
  "layers": {
    "roadsexdm2ben_roads_streetnameposition": {
      "fields": {
        "tid": {
          "src": "TID",
          "type": "String"
        },
        "street": {
          "src": "Street",
          "type": "String"
        },
        "namori": {
          "src": "NamOri",
          "type": "Real"
        }
      },
      "geometry_type": "Point",
      "src_layer": "RoadsExdm2ben.Roads.StreetNamePosition",
      "geom_fields": {
        "nampos": {
          "src": "NamPos",
          "type": "Point"
        }
      }
    },
    "roadsexdm2ben_roads_streetaxis": {
      "fields": {
        "tid": {
          "src": "TID",
          "type": "String"
        },
        "street": {
          "src": "Street",
          "type": "String"
        }
      },
      "geometry_type": "MultiLineString",
      "src_layer": "RoadsExdm2ben.Roads.StreetAxis",
      "geom_fields": {
        "geometry": {
          "src": "Geometry",
          "type": "MultiLineString"
        }
      }
    }
  }
  "..."
}
\end{srclst}

\subsection{Python Tabellenstrukturen}

\subsubsection{Petl}
Petl\footnote{http://petl.readthedocs.org/} ist eine Python \acs{etl} Bibliothek. Petl erlaubt es diverse Formate in eine Tabellenstruktur zu laden und bietet ein \acs{api} um darauf diverse Transformationen anzuwenden oder gar ganze Transformations-Pipelines aufzusetzen.

Der Vorteil von petl gegenüber anderen Lösungen liegt in dessen ``Lazy evaluation''. Es werden keine Daten geladen oder transformiert, bis das Resultat benötigt wird. Dann erst werden die Daten Zeile für Zeile eingelesen und durch die Transformations-Pipeline geschleust. Petl kann somit mit sehr grossen Datenmengen umgehen, ist aber im vergleich zur Konkurrenz langsamer und für Performance-kritische Applikationen ungeeignet \cite[Intro $\to$ Design goals]{petl}.


\subsubsection{Pandas}
Pandas ist zwar eine Bibliothek zur Datenanalyse, sozusagen das ``R'' für Python. Doch die zugrundeliegenden, ebenfalls Tabellen-artigen Datenstrukturen zusammen mit der \acs{api} von Pandas macht Transformationen wie Joins, Filtern, Umbenennungen und Berechnungen sowie String-Operationen sehr einfach. Pandas baut auf NumPy auf, der de-facto Standardbibliothek für Matrizen/Berechnungen in Python. Da NumPy in C bzw. Fortran und das Python \acs{api} sehr funktional-orientiert implementiert sind, lassen sich Operationen auf ganze Spalten oder Tabellen anwenden, was zu einer für Python-Verhältnisse sehr hohen Performance führt.


\begin{srclst}[label=src:pd:geopandas]{pycon}{Pandas mit GeoPandas}
>>> import geopandas
>>> gdf = geopandas.GeoDataFrame.from_file('Bahnhoefe.shp')
>>> gdf.columns
Index([u'CNTRYNAME', u'CONURB', u'DUP_NAME', u'LEVEL', u'NAME', u'NATION', u'PROV1NAME', u'TYPE', u'geometry'], dtype='object')
}
>>> gdf.CONURB[:10]
0         None
1      Chiasso
2      Balerna
3    Mendrisio
4     Capolago
5     Capolago
6     Capolago
7     Capolago
8     Capolago
9     Maroggia
>>> gdf[gdf.CONURB == 'Zürich'][:5]
        CNTRYNAME  CONURB DUP_NAME  LEVEL                  NAME  NATION  \
1317  Switzerland  Zürich        N     10     Sood Oberleimbach      41
1346  Switzerland  Zürich        N     10       Zürich Leimbach      41
1360  Switzerland  Zürich        N     10        Zürich Mannegg      41
1373  Switzerland  Zürich        N     10    Zürich Wollishofen      41
1374  Switzerland  Zürich        N     10  Zürich Tiefenbrünnen      41

     PROV1NAME  TYPE                                     geometry
1317    Zürich    30  POINT (681918.8961174991 241523.9264312923)
1346    Zürich    30  POINT (681750.8866669014 243289.5358952266)
1360    Zürich    30  POINT (681746.1775964648 244178.9977101207)
1373    Zürich    30  POINT (682765.2814791016 244804.6936600676)
1374    Zürich    30  POINT (685054.0860048197 244870.4588255175)
>>> gdf[gdf.geometry.within(shapely.geometry.Polygon([(704940,231559), (704949,231559),(704945,231550),]))]
        CNTRYNAME      CONURB DUP_NAME  LEVEL        NAME  NATION   PROV1NAME  \
1196  Switzerland  Rapperswil        N     10  Rapperswil      41  Ostschweiz

      TYPE                                     geometry
1196    30  POINT (704945.8093275279 231556.7192434487)
\end{srclst}


\subsubsection{PyTables/HDF}
PyTables ist eine high-performance Python-Bibliothek die wie Pandas ebenfalls auf NumPy aufbaut. Da das \acs{api} von PyTables im Vergleich zu Pandas eher low-level ist und weniger bietet wurde diese Bibliothek nicht weiter verfolgt.

\subsection{Fazit}

\cref{tab:pd:format-criteria} erläutert die wichtigsten Kriterien für die Bewertung der beschriebenen Lösungen.

\mytable{lXc}{
  \textbf{Kriterium} & \textbf{Beschreibung} & \textbf{Gewichtung} \\
  \midrule
  \textbf{Performance} & Wie performant ist die Lösung. Wichtig aufgrund des Effizienz \acs{nfr} welches beim Treffen mit Herrn Dorfschmid geäussert wurde. & \twoOfThree \\

  \textbf{Machbarkeit} & Die generelle Umsetzbarkeit der funktionalen Anforderungen mit dieser Lösung. & \threeOfThree \\

  \textbf{Aufwand / Komplexität} & Der benötigte Aufwand um Applikation der Lösung umzusetzen. & \twoOfThree \\

}{Bewertungskriterien Intermediäres Format}{pd:format-criteria}


\cref{tab:pd:format-results} fasst die verschiedenen Optionen zusammen.
\mytable{lXXXXX}{

  \textbf{Kriterium} & \textbf{Datenbank} & \textbf{ogrtools} & \textbf{Petl} & \textbf{Pandas} & \textbf{PyTables} \\

  \midrule

  \textbf{Performance} & \oneOfThree & ? & \oneOfThree & \twoOfThree & \oneOfThree \\

  \textbf{Machbarkeit} & \twoOfThree & \oneOfThree & \twoOfThree & \threeOfThree  & \twoOfThree \\

  \textbf{Aufwand / Komplexität} & \twoOfThree & \oneOfThree & \twoOfThree & \twoOfThree & \oneOfThree \\

  \midrule

  \textbf{Total} & \twoOfThree & \oneOfThree & \twoOfThree & \threeOfThree & \twoOfThree \\
}{Auswertung verschiedener Format-Varianten}{pd:format-results}


\begin{decision}[label=dec:pd:format]{Intermediäres Format}
Aufgrund der sowohl mächtigen \acs{api} wie auch der gleichzeitig hohen Performance und Integration mit Geo-Daten bzw. Shapely wurde für Pandas und deren Datenstrukturen entschieden. Die Variante in der Datenbank wurde obschon der Aufwand für die Transformationen komplett entfallen würde, aufgrund der Machbarkeit im Zusammenhang mit Echtzeit-Datenquellen verworfen.
\end{decision}

\section{Transformation}

\subsection{Anforderungen}
\xxx{gehört das wirklich hier hin? @chuesler}

Folgende Daten-Arten sind zu unterscheiden:
\begin{itemize}
\item Tabellarische Daten (z.B. CSV, Excel)
\item Baum-artige Daten (z.B. XML)
\item Unstrukturierte Daten (z.B. Bilder, unstrukturierter Text)
\end{itemize}

Diese Arbeit befasst sich aus Zeitgründen nur mit tabellarischen Daten.

In \cref{tab:pd:transformation-lang-req} werden die Anforderungen an die Transformations-Sprache zusammengefasst.

\mytable{lX}{
  \textbf{Anforderung} & \textbf{Beschreibung}\\
  \midrule
  \textbf{Mapping} & Felder müssen 1:1 auf andere Felder gemappt werden können (Feld umbenennen) \\
  \textbf{Default-Werte} & Neue Felder mit Default-Werten müssen hinzugefügt werden können \\
  \textbf{Joins} & Mehrere Datenquellen müssen miteinander verknüpft werden können. Dabei muss immer eine Join-Bedingung angegeben werden (keine karthesischen Produkte) \\
  \textbf{Filter/Prädikate} & Daten sollen gefiltert werden können \\
  \textbf{Erweiterbare Operationen} & Es sollen neue Operationen definiert werden können (in Python) \\
  \textbf{Sortierung} & Das Resultat der Transformation soll sortiert werden können\\
}{Anforderungen an die Transformations-Sprache/Konfiguration}{pd:transformation-lang-req}

\subsection{Datenbank-basiert: SQL}
Die Daten werden in einer Datenbank gespeichert, dort transformiert, und wieder herausgelesen.

Vorteile dieser Variante sind, dass die Abfrage-Sprache bereits vorhanden und bekannt ist und dass je nach verwendeter Datenbank auf vorhandene Funktionen und Optimierungen zurückgegriffen werden kann. Falls Postgres mit PostGIS zum Einsatz kommt, kann auch im Geo-Bereich eine umfassende Funktions-Bibliothek verwendet werden. 

Da die Daten jedoch vor der Transformation geparst, und anschliessend wieder formatiert werden müssen, entsteht ein beträchtlicher Performance-Verlust alleine durch das Abspeichern und Auslesen der Daten. Hinzu kommt, dass grosse Performance-Vorteile durch z.B. durch Indizes, nicht realisiert werden können, da nur eine einmalige Transformation stattfindet und daher die Kosten für die Erstellung des Indexes nicht amortisiert werden.

\subsection{ogrtools: ogrtransform}
Ogrtools beinhaltet bereits eine Transformations-Konfiguration namens ogrtransform. Aktuell werden aber diverse der gewünschten Features nicht unterstützt, wie z.B. Joins oder Filter. Ausserdem basiert die Implementation auf ogr2ogr, was die Daten-Arten, auf welche Transformationen angewendet werden können, massiv limitiert.

Da ogr2ogr nur auf Datei-Basis arbeitet, müssen die Daten, falls sie nicht in einem von ogr2ogr unterstützten Format vorliegen, erst geparst und formattiert und anschliessend ins Datei-System geschrieben werden. Nach der Transformation muss das Resultat wieder geparst werden, um in das vom Benutzer gewünschte Format konvertiert werden zu können. 

\subsection{DSL: Vereinfachtes SQL}
Dieses Format lehnt sich an SELECT aus SQL an. Es ist anzunehmen, dass Experten im Bereich Datentransformation sich mit SQL auskennen, was das Erlernen dieser Query-Sprache sehr vereinfachen würde. Dies wurde durch \prof bestätigt. \xxx[\prof rausnehmen?]

Da als Backend nicht eine SQL-Datenbank verwendet wird, sondern die Operationen auf Pandas DataFrames übersetzt werden, sind einige kleinere Differenzen zu SQL unvermeidbar. Zur Performance von Joins mit Pandas DataFrame, siehe \cite{pandasMergePerformance}.

Neue Funktionen können in Python definiert werden.

\subsection{DSL: Eigenes Format}
Diese Variante besitzt grundsätzlich die selben Eigenschaften wie eine SQL-basierte DSL. Dieser gegenüber entfällt jedoch das Familiaritäts-Argument. Ausserdem muss eine passende, einfach zu erlernende Syntax definiert werden.

\subsection{Fazit}
\cref{tab:pd:transformation-lang-criteria} erläutert die Kriterien für die Bewertung der verschiedenen Varianten.
\mytable{lXc}{
  \textbf{Kriterium} & \textbf{Beschreibung} & \textbf{Gewichtung} \\
  \midrule
  \textbf{Features} & Bereits vorhandene Unterstützung für die in \cref{tab:pd:transformation-lang-req} aufgelisteten Features & \oneOfThree \\
  \textbf{Geo-Daten} & Unterstützung für Geo-Daten & \threeOfThree \\
  \textbf{Allgemeine Daten} & Unterstützung für Nicht-Geo-Daten & \threeOfThree \\
  \textbf{Performance} & Performance der Transformation. Da diese Arbeit vor allem als Prototyp dienen soll ist dies nicht erste Priorität, soll aber auch nicht komplett Vernachlässigt werden. & \oneOfThree \\
  \textbf{Funktionen} & Erweiterbarkeit um eigene bzw. benutzerdefinierte Funktionen (in externer Sprache) & \twoOfThree \\
  \textbf{Familiarität} & Wie gut kennen Experten auf dem Gebiet Datentransformation diese Sprache, bzw. wie viel Lernaufwand ist notwendig, um die Sprache zu erlernen & \oneOfThree \\
}{Bewertungskriterien Transformationssprache}{pd:transformation-lang-criteria}
\xxx[Gewichtung der Kriterien abstimmen zwischen einzelnen Bereichen]
\cref{tab:pd:transformation-lang-results} fasst die verschiedenen Optionen zusammen.
\mytable{lXXXX}{
  \textbf{Kriterium} & \textbf{Datenbank: SQL} & \textbf{ogrtransform} & \textbf{DSL: SQL} & \textbf{DSL: Eigenes Format} \\
  \midrule
  \textbf{Features} & \threeOfThree & \oneOfThree & \threeOfThree & \threeOfThree \\
  \textbf{Geo-Daten} & \threeOfThree & \threeOfThree & \threeOfThree & \threeOfThree\\
  \textbf{Allgemeine Daten} & \threeOfThree & \oneOfThree & \threeOfThree & \threeOfThree\\
  \textbf{Performance} & \oneOfThree & \oneOfThree & \twoOfThree & \twoOfThree \\
  \textbf{Funktionen} & \twoOfThree & \oneOfThree & \threeOfThree & \threeOfThree \\
  \textbf{Familiarität} & \threeOfThree & \oneOfThree & \threeOfThree & \oneOfThree \\
  \midrule
  \textbf{Total} & \twoOfThree & \oneOfThree & \threeOfThree & \twoOfThree \\
}{Auswertung verschiedener Transformations-Sprachen}{pd:transformation-lang-results}
\xxx[@chuesler irgendwie überlappen die intermediäres format und sprache zu sehr. wo gehören z.B. performance und geo-daten hin? geht es hier nur um die sprache oder auch um die implementation dessen?]
Aufgrund dieser Auswertung fällt die Wahl auf eine SQL-basierte DSL.

\section{Frontend}
\subsection{Technologie}
\begin{decision}[label=dec:frontend:technology]{AngularJS als Frontendtechnologie}
Da \chuf und \fscf schon viel Erfahrung mit AngularJS gesammelt haben und dies ein sehr etabliertes Framework ist, haben wir uns für dieses entschieden. Evaluiert wurde es bereits im Rahmen den Studienarbeiten von \fscf und \rlif.
\end{decision}
\subsection{Bootstrap}
\mytable{lXXX}{

  \textbf{Kriterium} & \textbf{Bootstrap} & \textbf{Foundation} & \textbf{UIkit} \\

  \midrule

  \textbf{Bekanntheit} & \threeOfThree & \oneOfThree & \oneOfThree \\

  \textbf{Anz. Elemente} & \twoOfThree & \twoOfThree & \twoOfThree  \\

  \textbf{AngularJS} & \threeOfThree & \oneOfThree & \twoOfThree  \\

  \midrule

  \textbf{Total} & \threeOfThree & \oneOfThree & \twoOfThree  \\
}{Auswertung diverser Frontend Frameworks}{pd:frontend-bootstrap-results}
\begin{decision}[label=dec:frontend:bootstrap]{Bootstrap als Designgrundlage}
Aufgrund der Evaluation verschiedener Framework und der Vertrautheit mit Bootstrap, haben wir uns für die Verwendung von Twitter Bootstrap entschieden. Das Layout wird an Twitter Bootstrap angelehnt.
\end{decision}


\subsection{Menupunkte}
Die Menuführung soll schlicht und übersichtlich gehalten werden. Der Prototyp umfasst folgende Struktur: Daten - Daten bereitstellen - Transformation erstellen. \\
Unter dem Menupunkt Daten werden bestehende Dokumente und Transformationen bereitgestellt.\\

\subsection{Assisstent}
Es kann sich schwierig gestalten, eine Transformation von Grund auf zu erstellen. Deshalb soll eine Art Assistent zur Verfügung gestellt werden, welcher ein initiales ODHQL Query erstellen kann. Dieser soll Queries mit JOIN und UNION Operationen erstellen können.

\subsection{Drafts}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{fig/mockup_hcdi}
    \caption{Frühes Mockup ``Transformation erstellen''}
    \label{fig:pd:mockup-upload}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/Wireframes-Connectors}
    \caption{Frühes Idee Transformation mit ``Pipes'' erstellen. (Verworfen)}
    \label{fig:pd:connectors}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/Wireframes-Upload}
    \caption{Frühes Mockup für Upload Formular}
    \label{fig:pd:wireframe-upload}
\end{figure}
\chapter{Design und Implementation}
\section{Authentifizierung}
\xxx[@rliebi: kapitel design = IST zustand. hier hat es vergleiche]
In diesem Kapitel werden verscheidene Methoden der Authentifizierung beleuchtet.
\subsection{Herkömmliche Registrierung per E-Mail}
Die herkömmliche Registrierung bietet den Vorteil, dass sie keine Abhängigkeiten zu Dritt-Providern erfordert. So kann sich jeder Benutzer mit einer gültigen E-Mail Adresse einloggen. Ein Nachteil dieser Methode kann sein, dass Dummy-Adressen verwendet werden können und so ein registrierter Benutzer nicht wirklich Authentifiziert ist.
\subsection{oAuth2}
oAuth2 hat einen anderen Ansatz. Sociale Netzwerke bieten sich an, um mit den Login-Daten des bereits bestehenden Accounts in einem solchen ein Account bei opendatahub zu erstellen. So ist der Benutzer der sich registriert bereits authentifizert und es kann davon ausgegangen werden, dass es sich wirklich um diesen handelt. Die häufigkeit von``Fake-Profilen'' wird dadurch reduziert.

\subsection{Cookie-Based-Authentication}
Bei der Cookie-Based-Authentifizierung wird eine Session-ID auf der Seite des Clients in einem Server-Side Cookie gespeichert. Dieses wird mit jedem Request an den Server übermittelt. So kann der Server davon ausgehen, dass es sich beim Client um den Inhaber der Session ID handelt.

\subsection{Token-Based-Authentication}
Ein neuerer Ansatz, ist es, einen Signierten Token im Header jedes Requests der zum Server geht mitzusenden. Vorteile dieser Variante sind bestechend:
\begin{description}
  \item[Cross-Domain / CORS]
  Cookies und CORS spielt nicht so sauber zwischen mehreren Domains. Ein Token basierter Ansatz erlaubt es AJAX Aufrufe an jeden Server mit jeder Domain zu veranlassen, weil die Informationen im HTTP Header liegen.
  \item[Stateless (Server-Side-Scalability)]Es wird kein Session-Store benötigt. Der Token enthält alle Benutzerinformationen der Rest des `States' liegt im Local Storage auf der Client-Side.
  \item[Decoupling] Man ist nicht an ein Authentifizierungs-Schema gebunden. Der Token kann überall generiert werden, sprich die API kann von überall her aufgerufen werden.
  \item[Mobile-Ready]Cookies sind in nativen mobilen Apps böse.
  \item[CSRF]Cross-Site Requests stellen kein Problem dar. Es gibt kein Authentication Cookie, das wiederverwendet werden könnte.
  \item[Standard]Es gibt bereits einen Standard. (\gls{JWT})
\end{description}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/cookie-token-auth}
    \caption{Vgl. Cookie vs. Token Auth}
    \label{fig:pd:cookie-token-auth}
\end{figure}
\section{Architektur}
\xxx
Daten-Lieferanten liefern ihre Daten per Web-Interface oder REST-API. Diese Daten werden in der Datenbank gespeichert und im Web-Interface angezeigt. Daten-Nutzer rufen diese Daten wieder über das Web-Interface oder per REST-API ab. 

Dritt-Entwickler können eigene Komponenten entwickeln und beisteuern, wie in \cref{fig:pd:arch-overview} zu sehen ist:
\begin{description}
\item[Parser] nehmen Daten entgegen und bereiten sie zur Weiterverarbeitung auf
\item[Formatter] formatieren Daten in das vom Benutzer gewünschte Format
\end{description}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/ODH-Architecture-Overview}
    \caption{Grobe Architektur-Übersicht}
    \label{fig:pd:arch-overview}
\end{figure}

\subsection{Komponenten}


\subsection{Klassen}


\subsection{Sequenzdiagramm}


\section{Transformationssprache ODHQL}

\subsection{Syntax}

Die Syntax von \acs{odhql} orientiert sich stark an SELECT aus ANSI SQL.

Eine formale Beschreibung der Transformations-Sprache ist in \cref{app:odhql-syntax} zu finden.

\subsection{Unterstützte Features}

\cref{tab:pd:transformation-lang-impl} beschreibt die unterstützten Features:
\mytable{lX}{
  \textbf{Anforderung} & \textbf{Beschreibung}\\
  \midrule
  \textbf{Mapping} & Felder können mit oder ohne Alias angegeben werden. Falls kein Alias vorhanden ist wird der Feldname beibehalten. Es ist möglich, Sonder- und Leerzeichen zu verwenden, indem der Feldname in doppelten Anführungszeichen geschrieben wird. \\
  \textbf{Default-Werte} & Unterstützt werden Integer, Float, sowie Strings (in einfachen Anführungszeichen). \\
  \textbf{Joins} & Es muss immer mindestens eine Datenquelle angegeben werden. Falls mehr verwendet werden sollen muss mindestens eine Join-Bedingung vorhanden sein (es werden mehrere Join-Bedingungen unterstützt, jedoch nur per ``and''-Verknüpung). Neben Inner Joins werden auch Left, Right und Full Outer Joins unterstützt. \\
  \textbf{Filter/Prädikate} & Vorhandene Filter beinhalten: ``is null'', ``in (...)'', relationale Operatoren wie ``='', ``<'', etc, Prädikate und ``like `regex'\ '' (letzteres ist eine Abweichung von ANSI SQL, welches eine separate Syntax für Like-Filter hat) \\
  \textbf{Erweiterbare Funktionen} & Alle Funktionen werden über eine Function Registry in Python aufgerufen und können daher einfach erweitert werden. \\
  \textbf{Sortierung} & Es kann eine Order By-Klausel angegeben werden. Falls Unions verwendet werden muss die Order By-Klausel am Ende angegeben werden, nicht pro Query. \\
  \textbf{Unions} & Mehrere Queries können per Union zusammengefügt werden. Die Implementation entspricht dem UNION ALL aus SQL, es findet also keine Deduplizierung statt. \\
}{Durch \acs{odhql} implementierte Features}{pd:transformation-lang-impl}

Jede Transformation definiert eine Liste von Feldern oder Werten, sowie mindestens eine Datenquelle. 

\subsection{Beispiele}
Folgende Items sind in der Feld-Liste erlaubt:
\begin{description}
\item[Feld] Name des Feldes, mit dem Namen oder Alias der Datenquelle als Präfix. Feld-Namen und Aliases können in doppelten Anführungszeichen geschrieben werden, z.B. wenn Sonder- oder Leerzeichen verwendet werden sollen. Zu beachten ist, dass Feld-Namen immer einen Präfix brauchen, auch ausserhalb der Feld-Liste.
\begin{src}{sql}
a.beschreibung, a.geom as geometry, a."Zentroid X [m]"
\end{src}
\item[Wert] Integer, Float, Boolean, Null oder String in einfachen Anführungszeichen. Es muss zwingend ein Alias angegeben werden. 
\begin{src}{sql}
1 as one, 3.14 as pi, false as active, 'sun' as local_star
\end{src}
\item[Funktion] Ein Funktionsaufruf besteht aus einem Namen sowie einer Liste von Argumenten in runden Klammern. Funktionsaufrufe können verschachtelt werden.
\begin{src}{sql}
 cast(a.pi, 'float') as pi,
 concat('POINT(', cast(v0.x, 'string'), ' ', cast(v0.y, 'string'), ')')) as geometry
\end{src}
\end{description}

Datenquellen können per Join verknüpft werden.
\begin{src}{sql}
  from "Gebäude" as geb join Strassen as str on geb.str_id = str.id
\end{src}

Unterstützt werden Inner Join, Left, Right und Full Outer Join. Dabei wird jeweils die einfachste Syntax\footnote{\texttt{join}, \texttt{left join}, \texttt{right join} und \texttt{full join}.} verwendet.

Als Filter können folgende Ausdrücke verwendet werden:
\begin{description}
\item[Relationale Operatoren] Unterstützt werden die Operatoren `=', `!=', `<', `>', `<=', `>='.
\begin{src}{sql}
a.nr > 4, a.active = false
\end{src}
\item[Like] Anders als bei SQL verwendet der Like-Operator von \acs{odhql} implementationsbedingt Reguläre Ausdrücke (Python Syntax\footnote{Siehe \url{https://docs.python.org/2/library/re.html\#regular-expression-syntax}}).
\begin{src}{sql}
a.beschreibung like 'Z[uü]rich', a.beschreibung not like 'Z[uü]rich'
\end{src}
\item[In] Sucht den Ausdruck in einer Liste von Elementen.
\begin{src}{sql}
a.nr in (3, 4, 5), a.nr not in (2, 6, 7)
\end{src}
\item[Null-Check] Prüft, ob ein Ausdruck Null (bzw. None) ist.
\begin{src}{sql}
a.optional_field is null, a.mandatory_field is not null
\end{src}
\end{description}

Die Resultate mehrerer Queries werden mit Union zusammengehängt. Dies verhält sich wie ``union all'' in SQL, d.h. es findet keine Deduplizierung der Daten statt.
\begin{src}{sql}
  select a.description, a.geometry from A as a
   union
  select b.description, b.geometry from B as b
\end{src}
Zu beachten ist, dass die Feld-Listen der Queries kompatibel sein müssen.
\newline

\subsection{Abstrakter Syntax-Baum (AST)}
Der ODHQL-Parser analysiert die Abfrage in Text-Form und wandelt diese in einen Abstrakten Syntax Baum (AST) um. Dieser wird anschliessend vom Interpreter verwendet, um die Transformation auf die Daten anzuwenden.

\cref{fig:pd:odhql-ast-toplevel} zeigt die Toplevel-Struktur des Abstrakten Syntax-Baums.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{fig/odhql-ast-union.pdf}
\caption{AST: Unions}
\label{fig:pd:odhql-ast-toplevel}
\end{figure}

Die in \cref{fig:pd:odhql-ast-expressions} beschriebenen Ausdrücke repräsentieren Felder, Werte, Funktionsaufrufe, etc.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{fig/odhql-ast-expression.pdf}
\caption{AST: Ausdrücke}
\label{fig:pd:odhql-ast-expressions}
\end{figure}

\cref{fig:pd:odhql-ast-datasources} zeigt auf, wie Datenquellen repräsentiert werden.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{fig/odhql-ast-datasources.pdf}
\caption{AST: Datenquellen}
\label{fig:pd:odhql-ast-datasources}
\end{figure}

Filter werden wie in \cref{fig:pd:odhql-ast-filter} beschrieben dargestellt.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{fig/odhql-ast-filter.pdf}
\caption{AST: Filter}
\label{fig:pd:odhql-ast-filter}
\end{figure}

\subsection{Interpreter}
\xxx[fscala]
Mit Pandas und NumPy ist bereits eine API für viele SQL-ähnliche Operationen wie Selektion, Joins und diverse String- und Geometriefunktionen (GeoPandas Erweiterung) vorhanden. Die Herausforderung bei der Entwicklung des Interpreters liegt somit einerseits in einer performanten und möglichst fehlerfreien Ausführung sowie der Erweiterbarkeit durch eigene Funktionen.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{fig/odhql-flow.pdf}
\caption{High-level Ablauf der Abfrage-Interpretation}
\label{fig:pd:odhql-flow}
\end{figure}


\subsection{Funktionen}
Die Funktionen der \acs{odhql} wurden möglichst erweiterbar implementiert. Jede Funktion ist als Python Klasse zu implementieren, welche von der Klasse \path{VectorizedFunction} erben und die Methode \path{apply} zu implementieren hat. Durch die Vererbung und der zugrundeliegenden Python-Metaklasse wird die neue Funktion automatisch registriert und ist sofort mit dem Klassennamen verfügbar. Der Name kann jedoch durch das statische Attribut \path{name} auch explizit gesetzt werden. Die Anzahl Funktionsparameter werden vom Interpreter bzw. von der Execution-Engine der Funktionen via Reflection sichergestellt. Für die Überprüfung der Argumente ist jede Funktion selbst zuständig. Die Parent-Klasse bietet jedoch bereits einige Assertion-Methoden z.B. für Datentypen, Listen und Geometrien.

Ein Beispiel einer solchen Funktion ist in \cref{src:pd:odhql-function} ersichtlich. Hierbei wird nach der Überprüfung der Argumente direkt an die Pandas-Methode \path{str.pad} weiterdelegiert.


\begin{srclst}[label=src:pd:odhql-function]{python}{Beispiel-Implementation der \acs{odhql} Funktion \texttt{PAD()}}
from hub.odhql.functions.core import VectorizedFunction

class Pad(VectorizedFunction):
    name = 'PAD'

    def apply(self, strings, width, side):
        self.assert_str('string', strings)
        self.assert_in('side', side, ['left', 'right', 'both'])
        self.assert_int('width', width)
        return strings.str.pad(width, side)
\end{srclst}


\section{Caching}
Aufgrund der Entscheidung alle Daten Verlustfrei im Originalformat zu speichern sowie der Verwendung von In-Memory Tabellen-artigen Datenstrukturen, muss sichergestellt werden, dass nicht bei jeder Operation die Originaldaten von Format-Parser neu eingelesen werden müssen.

Aus diesem Grund wurde ein dreistufiges Caching-Konzept für die gesamte Applikation implementiert. Es werden die Caching-Mechanismen von Django mit eingen Modifikationen/Erweiterungen verwendet. Aus diesem Grund können die Caching-Backends jederzeit durch beliebige von Django Unterstützte Backends wie z.B. Memcached, Filesystem, etc. verwendet werden.

\subsection{Caches}

\xxx[ref]
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{fig/caching.pdf}
\caption{Übersicht der Caches}
\label{fig:pd:caches}
\end{figure}

\xxx[ref]
\mytable{lX}{
  \textbf{Cache} & \textbf{Beschreibung}\\
  \midrule
  Level 1 & Der Level 1 Cache ist ein In-Memory Cache der Applikation und sehr kurzlebig (\SI{60}{\second}). Dieser wird hauptsächlich verwenden um Zwischenresultate wie Formatkonversionen oder Resultate einer Abfrage zu speichern, damit der Benutzer im Webfrontend die Daten sofort mittels Pagination anschauen kann.\\

  Level 2 & Im Cache zweiter Stufe werden hauptsächlich Daten des Level 3 Caches für mehrere Minuten zwischengespeichert um den Overhead der Abfrage inkl. potenziellem Transfer mehrerer Megabytes sowie Deserialisierung der Python Objekte aus der Datenbank zu vermeiden.\\

  Level 3 & Im persistenten Cache werden eingelesene Dateien im intermediären Format (d.h. DataFrame Objekte) sowie gespeicherte Transformationen ohne Online-Datenquellen persistiert.\\
}{Beschreibung der Caches}{pd:caches}

\subsection{Invalidierung}
\xxx[fscala: cache key]

\section{Format-Unterstützung}
\subsection{Unterstützte Formate}
\cref{tab:pd:formats} beschreibt die implementierten Formate.

\mytable{lX}{
  \textbf{Format} & \textbf{Implementiert via}\\
  \midrule
  CSV (inkl. GeoCSV) & (Geo)Pandas \\
  Excel (xls, xlsx) & Pandas \\
  GML & ogr2ogr \\
  GeoJSON & GeoPandas \\
  GeoPackage & GeoPandas \\
  Interlis 1 & ogr2ogr \\
  Interlis Modell (ili) & custom \\
  JSON & Pandas \\
  KML  & fastkml, custom \\
  ESRI Shapefile & GeoPandas \\
  WFS (nur Parser) & ogr2ogr \\
  XML & custom \\
}{Beschreibung der implementierten Formate}{pd:formats}
\xxx[figure out footnotes in that env]
% \footnote{deaktiviert, siehe \cref{sec:pd:format-gdal-problems}}

GeoPandas verwendet (via Fiona) ebenfalls GDAL.

\subsubsection{Probleme mit GDAL}\label{sec:pd:format-gdal-problems}
Für einen grossen Teil der Formate wird GDAL verwendet, entweder via GeoPandas/Fiona oder direkt per ogr2ogr. 

Folgende GDAL-Versionen stehen zum aktuellen Zeitpunkt zur Verfügung:
\begin{itemize}
\item 1.10.1
\item 1.11.0 - 1.11.2
\item 2.0.0beta1
\end{itemize}

Die Entwicklungsumgebung bestand aus einer virtuellen Maschine mit Ubuntu 14.04 LTS, für welches ein GDAL-Package in Version 1.10.1 vorhanden ist. 

Der Betreuer wünschte jedoch, dass wenn möglich Unterstützung für das relativ neue Format GeoPackage eingebaut wird. GDAL 1.10.1 unterstützt GeoPackage nicht.

Im Versuch, diese Unterstützung einzubauen wurde GDAL selbst kompiliert. Dem Betreuer waren Probleme mit GDAL 1.11.2 bekannt, weswegen 1.11.1 gewählt wurde.

In Tests wurden folgende Probleme gefunden:
\begin{itemize}
\item In GDAL 1.11.0 bis 2.0.0beta1 führt der Versuch, nach Interlis 1 zu konvertieren, zu einem Segmentation Fault. Das Problem wurde vom Betreuer weitergemeldet. In aktuellen nightly-Versionen soll dies behoben sein - dies wurde nicht verifiziert.
\item Der GeoPackage-Treiber wurde in GDAL 1.11.0 hinzugefügt. Leider kann er in dieser Version nur als unausgereift bezeichnet werden:
  \begin{itemize}
  \item Layer-Namen werden ohne Bereinigung oder Maskierung als Tabellen-Namen verwendet, was zu Exceptions seitens SQLite führt.
  \item Wenn die Ursprungs-Daten bereits eine Spalte namens ``fid'' beinhalten, bricht die Konvertierung ab wegen doppelter Spalten-Namen, da der Treiber diese selbst nochmals hinzugeneriert.
  \item Reine Daten-Tabellen werden nicht unterstützt. Diese sind zwar nicht Bestandteil des reinen GeoPackage-Standards, können aber mit Extended GeoPackage realisiert werden. Dies ist in GDAL erst ab Version 2.0.0 möglich.
  \end{itemize}
\end{itemize}

GeoPackage-Support wurde in GDAL 2.0.0 massiv verbessert. Diese Version ist bei Abschluss der Arbeit jedoch noch in der Beta-Phase, weshalb diverse unserer Abhängigkeiten noch nicht dazu kompatibel sind.

\xxx[Reword. I'm srsly bad at this o.O]
\begin{decision}[label=dec:pd:gdal-version]{GDAL-Version}
Zum Abgabe-Zeitpunkt sind folgende GDAL-Versionen verfügbar: 1.10.1, 1.11.0 - .2 und 2.0.0beta1.

Wegen Problemen mit mehreren Treibern entfallen die Versionen 1.11.x.

Version 2.0.0beta1 entfällt wegen mangelnder Unterstützung in unseren Abhängigkeiten.

Daher wird vorerst GDAL 1.10.1 eingesetzt.
\end{decision}

\subsubsection{Interlis 2}
Aufgrund von Problemen mit dem Format-Support in ogr2ogr wurde in Absprache mit dem Betreuer entschieden, Unterstützung für Interlis 2 nicht zu implementieren. Siehe dazu das Sitzungsprotokoll vom 15.04.2015.

\subsubsection{Erweiterung}
Die Unterstützung für neue Formate kann relativ einfach hinzugefügt werden. Dazu müssen Subklassen für folgende Python-Klassen erstellt werden:

\begin{description}
\item[hub.formats.core.Format] Dient zur Identifikation des Formats von Datenquellen (z.B. Dateien)
\item[hub.formats.core.Formatter] Erhält DataFrames und produziert Dateien, welche der Benutzer herunterladen kann.
\item[hub.formats.core.Parser] Erhält Daten (aus Dateien, WFS, ...) und produziert DataFrames, welche von ODH weiterverwendet werden können.
\end{description}

Für die Implementation via ogr2ogr stehen im Modul hub.formats.geobase Hilfsklassen zur Verfügung.

Durch Vererbung und die verwendete Python-Metaklasse wird das neue Format automatisch registriert und kann sofort verwendet werden.

Ein Beispiel ist in \cref{src:pd:format-example} zu sehen. Darin wird die Unterstützung für ESRI Shapefile implementiert. Der Formatter verwendet hierzu die Hilfsklasse GeoFormatterBase, der Parser jedoch direkt GeoPandas.

\begin{srclst}[label=src:pd:format-example]{python}{Beispiel-Implementation der Format-Unterstützung für ESRI Shapefile}
import geopandas as gp
import os

from hub.formats import Format, Parser
from hub.formats.geobase import GeoFormatterBase
from hub.utils import ogr2ogr

if 'ESRI Shapefile' in ogr2ogr.SUPPORTED_DRIVERS:
    class Shapefile(Format):
        label = 'ESRI Shapefile'
        ogr_format = ogr2ogr.SHP

        description = """
        Ein ursprünglich für die Firma ESRI entwickeltes Format für Geodaten.
        """

        extension = 'shp'

        @classmethod
        def is_format(cls, file, *args, **kwargs):
            return file.extension == 'shp'

    class ShapefileFormatter(GeoFormatterBase):
        targets = Shapefile,

        supported_types = {'Point', 'LineString', 'LinearRing', 'Polygon', 'MultiPoint'}

        @classmethod
        def format(cls, dfs, name, format, *args, **kwargs):
            return super(ShapefileFormatter, cls).format(dfs, name, format, 'ESRI Shapefile', 'shp', *args, **kwargs)

    class ShapefileParser(Parser):
        accepts = Shapefile,

        @classmethod
        def parse(cls, file, format, *args, **kwargs):
            with file.file_group.on_filesystem() as temp_dir:
                return gp.read_file(os.path.join(temp_dir, file.name), driver='ESRI Shapefile')
\end{srclst}


\section{API}
Die Webapplikation kommuniziert mit dem Server über ein \gls{rest} \gls{api}.

\subsection{Konzepte}
Folgende Konzepte werden von mehreren Endpunkten verwendet und werden daher separat aufgelistet.

\subsubsection{Paging} \label{sec:pd:api-paging} 
Paging ermöglicht das limitieren der Anzahl Resultate, welche eine Abfrage liefert. Die Einträge pro Seite werden mit dem Parameter \texttt{count} gesteuert, die Seite mit \texttt{page}.

\subsubsection{Suche} \label{sec:pd:api-search} 
Für die Suche werden folgende Parameter verwendet:
\mytable{lX}{
  \textbf{Parameter} & \textbf{Beschreibung}  \\
  \midrule
  \textbf{filter[name]} & Filtert anhand des Namens \\
  \textbf{filter[description]} & Filtert anhand der Beschreibung \\
  \textbf{filter[search]} & Filtert sowohl mit Namen als auch mit der Beschreibung \\
  \textbf{filter[mineOnly]} & Liefert nur Dokumente, die dem eingeloggten Benutzer zugeordnet sind \\
}{Such-Parameter}{pd:api-search}

\subsubsection{Sortierung} \label{sec:pd:api-sort}
Sortiert die Resultate der Anfrage. Dazu kann der Parameter \texttt{sorting[key]=asc|desc} verwendet werden, mit folgenden Werten als \texttt{key}: \texttt{name}, \texttt{description}, \texttt{private}, \texttt{owner} und \texttt{created\_at}.

\paragraph{Preview} \label{sec:pd:api-preview}
Liefert eine Vorschau der Daten. Enthalten ist der Unique Name, welcher in Abfragen verwendet werden kann, eine Liste der Felder und deren Typen sowie ein paar Daten-Zeilen. Die Anzahl der gelieferten Daten-Zeilen kann per Paging gesteuert werden (siehe \cref{sec:pd:api-paging}).

\begin{srclst}{json}{Preview für ein Dokument}
[  
   {  
      "type":"preview",
      "unique_name":"ODH4_Bahnhoefe",
      "parent":"4",
      "types":{  
         "TYPE":"BIGINT",
         "fid":"TEXT",
         "CNTRYNAME":"TEXT",
         "DUP_NAME":"TEXT",
         "PROV1NAME":"TEXT",
         "LEVEL":"BIGINT",
         "geometry":"GEOMETRY",
         "NATION":"BIGINT",
         "CONURB":"TEXT",
         "name":"TEXT"
      },
      "url":"http://beta.opendatahub.ch/api/v1/document/4/preview/",
      "columns":[ "TYPE", "fid", "geometry", "DUP_NAME", "PROV1NAME", "LEVEL", "CNTRYNAME", "NATION", "CONURB", "name" ],
      "data":[  
         {  
            "TYPE":"3",
            "fid":"F0",
            "CNTRYNAME":"Switzerland",
            "DUP_NAME":"N",
            "PROV1NAME":"Tessin",
            "LEVEL":"10",
            "geometry":"POINT (45.90247783251107 76645.7742365048)",
            "NATION":"41",
            "CONURB":"NULL",
            "name":"Chiasso Rail"
         },
         ...
      ],
      "count":1781,
      "name":"Bahnhoefe"
   }
]
\end{srclst}

\subsubsection{Download} \label{sec:pd:api-download}
Fordert einen Download der Daten an. Das gewünschte Format wird mit dem Parameter \texttt{fmt} angegeben. Gültige Werte sind die von \cref{sec:pd:api-format} gelieferten Namen. Ohne Format-Angabe werden die Original-Daten geliefert.

Hinweis: Dokumente können nicht als ganzes heruntergeladen werden. Stattdessen müssen die File Groups für diesen Zweck verwendet werden.

\subsection{Root - GET /api/v1/}
Als Prefix für die \gls{api} URLs wird \url{/api/v1/} verwendet, was eine spätere versionierte Erweiterung erlaubt.
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{fig/api-root}
\caption{API: Übersicht}
\label{fig:pd:api-root}
\end{figure}

Da für die Implementation des \gls{api} die Django-Erweiterung \texttt{rest\_framework}\footnote{\url{http://www.django-rest-framework.org/}} verwendet wurde, kann die API gut auch manuell ausprobiert werden. Dabei ist zu beachten, dass die Schrägstriche am Ende der URLs nicht optional sind.

\subsection{Konfiguration - GET /api/v1/config/}
Gibt die für den HTML5 Client notwendigen Konfigurationsinformationen zurück.

\begin{srclst}{json}{Beispielantwort für GET /api/v1/config/}
{
  "GITHUB_PUBLIC": "8ef558ed3fb0f5385da5", 
  "FACEBOOK_PUBLIC": "401520096685508", 
  "PACKAGE_PREFIX": "ODH", 
  "TRANSFORMATION_PREFIX": "TRF"
}
\end{srclst}

\subsection{Liste der unterstützten Formate - GET /api/v1/format/} \label{sec:pd:api-format}
Liefert die Liste der unterstützten Formate. Neue Formate erscheinen automatisch, sobald sie registriert sind, vorausgesetzt dass ein Formatter für das Format existiert.

\begin{srclst}{json}{Beispielantwort für GET /api/v1/format/}
[
    {
        "name": "JSON", 
        "label": "JSON", 
        "description": "JavaScript Objekt-Notation. Nützlich zur Wiederverwendung in Webapplikationen.", 
        "example": "", 
        "extension": "json"
    }, 
    {
        "name": "GML", 
        "label": "GML", 
        "description": "Geometry Markup Language (GML) ist ein Dateiformat zum Austausch von Geodaten.", 
        "example": "", 
        "extension": "gml"
    }, 
    ...
]
\end{srclst}

\subsection{Packages}
``Package'' wird als Überbegriff für Dokumente, welche von Nutzern hochgeladen wurden, und Transformationen verwendet. Packages haben Attribute wie Name und Beschreibung, können eine Vorschau erzeugen und der Benutzer kann einen Download anfordern.

\subsubsection{Package-Liste - GET /api/v1/package/}\label{sec:pd:api-package-list}
Liefert eine Liste der Packages.

\begin{srclst}{json}{Beispielantwort für GET /api/v1/package/}
{
    "count": 37, 
    "next": "http://beta.opendatahub.ch/api/v1/package/?page=2", 
    "previous": null, 
    "results": [
        {
            "id": 1, 
            "url": "http://beta.opendatahub.ch/api/v1/package/1/", 
            "name": "Test mockaroo.com.csv", 
            "description": "Testdaten (Originalformat: CSV)", 
            "private": false, 
            "owner": {
                "id": 1, 
                "username": "testuser", 
                "first_name": "", 
                "last_name": ""
            }, 
            "created_at": "2015-06-01T12:35:31.480135Z", 
            "type": "document", 
            "preview": "http://beta.opendatahub.ch/api/v1/document/1/preview/", 
            "template": false
        }, 
        {
            "id": 2001, 
            "url": "http://beta.opendatahub.ch/api/v1/package/2001/", 
            "name": "TROBDB: Baustellen Februar 2015", 
            "description": "TROBDB: Baustellen Februar 2015", 
            "private": false, 
            "owner": {
                "id": 1, 
                "username": "testuser", 
                "first_name": "", 
                "last_name": ""
            }, 
            "created_at": "2015-06-01T12:35:45.794037Z", 
            "type": "transformation", 
            "preview": "http://beta.opendatahub.ch/api/v1/transformation/2001/preview/", 
            "template": false
        },
        ...
    ]
}
\end{srclst}

Dieser Endpunkt unterstützt Paging (\cref{sec:pd:api-paging}), Suche (\cref{sec:pd:api-search} und Sortierung (\cref{sec:pd:api-sort}).

\subsubsection{Package-Details - GET /api/v1/package/<id>/}\label{sec:pd:api-package-details}
Liefert die Details zu einem Package. Dies sind grundsätzlich die selben Informationen wie in der Package-Liste, jedoch um einige weitere Details ergänzt.

Die Antwort sieht unterschiedlich aus, je nach dem ob das Package ein Dokument (\cref{pd:api-package-document}) oder eine Transformation (\cref{pd:api-package-transformation}) ist.

\begin{srclst}[label=pd:api-package-document]{json}{Beispielantwort für ein Dokument}
{
    "id": 4, 
    "url": "http://beta.opendatahub.ch/api/v1/document/4/", 
    "name": "Test Bahnhoefe.gml, Bahnhoefe.gfs, Bahnhoefe.xsd", 
    "description": "Testdaten (Originalformat: GML)", 
    "file_groups": "http://beta.opendatahub.ch/api/v1/document/4/filegroup/", 
    "private": false, 
    "owner": {
        "id": 1, 
        "username": "testuser", 
        "first_name": "", 
        "last_name": ""
    }, 
    "created_at": "2015-06-01T12:35:32.241407Z", 
    "preview": "http://beta.opendatahub.ch/api/v1/document/4/preview/", 
    "type": "document"
}
\end{srclst}

\begin{srclst}[label=pd:api-package-transformation]{json}{Beispielantwort für eine Transformation}
{
    "id": 2001, 
    "url": "http://beta.opendatahub.ch/api/v1/transformation/2001/", 
    "name": "TROBDB: Baustellen Februar 2015", 
    "description": "TROBDB: Baustellen Februar 2015", 
    "transformation": "SELECT t.\"Federführende_Stelle\" as userid,
           substring(t.Bauarbeiten, 1, 100) as title,
           t.Bauarbeiten as description,
           to_char(to_date(concat(extract(t.Dauer, '^((?:\\\\d\\\\d?\\\\.){2})'), nvl(extract(t.Dauer, '(\\\\d{4}) -'), extract(t.Dauer, '(\\\\d{4})$'))), '%d.%m.%Y'), '%Y-%m-%d') as trob_start, -- add year from end date if not present
           to_char(to_date(extract(t.Dauer, '- ([\\\\d\\\\.]+)'), '%d.%m.%Y'), '%Y-%m-%d') as trob_end,
           null as trob_interval,
           'both' as direction,
           null as diversion_advice,
           'CH' as country,
           t.Bauarbeiten as reason,
           t.Strecke as object_name,
           'street' as object_type,
           'closed' as trob_type,
           ST_SetSRID(ST_GeomFromText(CONCAT('POINT(', CAST(t.\"Zentroid X [m]\", 'text'), ' ',CAST(t.\"Zentroid Y [m]\", 'text'), ')')), 21781) as geometry
           from \"ODH8_Baustellen Februar 2015\" as t", 
    "private": false, 
    "owner": {
        "id": 1, 
        "username": "testuser", 
        "first_name": "", 
        "last_name": ""
    }, 
    "data": "http://beta.opendatahub.ch/api/v1/transformation/2001/data/", 
    "is_template": false, 
    "preview": "http://beta.opendatahub.ch/api/v1/transformation/2001/preview/", 
    "referenced_file_groups": "http://beta.opendatahub.ch/api/v1/transformation/2001/filegroups/", 
    "referenced_transformations": "http://beta.opendatahub.ch/api/v1/transformation/2001/transformations/", 
    "type": "transformation"
}
\end{srclst}

\paragraph{File Groups} Die \texttt{file\_group}-Url liefert die Liste der File Groups, die zu diesem Dokument gehören. Siehe auch \cref{sec:pd:api-filegroups}.

\paragraph{Preview} Die Preview-Url liefert eine Vorschau der Daten im Package. Siehe auch \cref{sec:pd:api-preview}.

\paragraph{Referenced File Groups/Transformations} Dies sind Urls für File Groups bzw. Transformations, welche in dieser Transformation verwendet wurden.

\paragraph{Template} Templates sind abstrakte Transformationen, welche als Vorlage für konkrete Transformationen dienen. Für Templates kann keine Vorschau erstellt werden.

\paragraph{Data} Url für den Download der Daten. Siehe \cref{sec:pd:api-download} für Details.

\subsection{Dokumente}
\subsubsection{Dokument-Liste - GET /api/v1/document/}
Die Dokument-Liste ist identisch zur Package-Liste, mit der Ausnahme, dass nur Dokumente aufgeführt werden. Siehe \cref{sec:pd:api-package-list} für Details.

\subsubsection{Dokument-Details - GET /api/v1/document/<id>/} \label{sec:pd:api-document-details}
Identisch zu Package-Details, mit der Ausnahme, das nur Dokumente unterstützt werden. Siehe \cref{sec:pd:api-package-details} für Details.

\subsubsection{Dokument-Erstellung - POST /api/v1/document/}
Erstellt ein neues Dokument. 

\cref{tab:pd:api-offer} führt die erwarteten Parameter auf.
\mytable{lX}{
  \textbf{Parameter} & \textbf{Beschreibung} \\
  \midrule
  \textbf{name} & Dokument-Name (max. 200 Zeichen). \\
  \textbf{description} & Beschreibung des Dokuments (keine Limitation). \\
  \textbf{private} (default: false) & Steuert ob das Dokument für andere Benutzer sichtbar ist.\\
  \textbf{format} (default: null) & Format der hochgeladenen Daten. Wenn nicht angegeben wird versucht, das Format automatisch zu erkennen. \\
}{Parameter für Dokument-Erstellung}{tab:pd:api-document-offer}

Zusätzlich wird eine der folgenden Optionen benötigt:
\begin{description}
\item[file] Direkter Datei-Upload. In diesem Fall wird die Anfrage als Multipart Form Request erwartet. Es kann mehr als eine Datei auf einmal hochgeladen werden. Diese werden dann dem selben Dokument zugeordnet, nach Datei-Namen (ohne Erweiterung) in File Groups gruppiert.
\item[url] Erstellt ein Dokument mit einer Url als Datenquelle. Dies kann entweder ein \gls{wfs}-Endpoint oder eine http-Url sein. Zusätzlich kann der Parameter \texttt{refresh} angegeben werden, welcher steuert wie oft die Daten abgerufen werden sollen (in Sekunden). Erlaubt sind Werte von 60 (1 Minute) bis 86400 (1 Tag).
\end{description}

\begin{srclst}{json}{Beispielanfrage für POST /api/v1/document/}
{
  "name":"aasdfas",
  "description":"asdfasdf",
  "url":"http://maps.zh.ch/wfs/TbaBaustellenZHWFS",
  "refresh":3600
}
\end{srclst}

\subsubsection{Dokument-Update - PUT /api/v1/document/<id>/}
Aktualisiert ein Dokument. Nur die Meta-Daten (Name, Beschreibung, Privat) können aktualisiert werden.

Nur der Eigentümer eines Dokuments kann es aktualisieren.

\subsection{Transformationen}
\subsubsection{Transformations-Liste - GET /api/v1/transformation/}
Die Transformations-Liste ist identisch zur Package-Liste, mit der Ausnahme, dass nur Transformationen aufgeführt werden. Siehe \cref{sec:pd:api-package-list} für Details.

\subsubsection{Transformations-Details - GET /api/v1/transformation/<id>/}
Identisch zu Package-Details, mit der Ausnahme, das nur Transformationen unterstützt werden. Siehe \cref{sec:pd:api-package-details} für Details.

\subsubsection{Transformation erstellen - POST /api/v1/transformation/}
Erstellt eine neue Transformation.

\cref{tab:pd:api-transformation-create} führt die erwarteten Parameter auf.
\mytable{lX}{
  \textbf{Parameter} & \textbf{Beschreibung} \\
  \midrule
  \textbf{name} & Name der Transformation (max. 200 Zeichen). \\
  \textbf{description} & Beschreibung der Transformation (keine Limitation). \\
  \textbf{private} (default: false) & Steuert ob die Transformation für andere Benutzer sichtbar ist.\\
  \textbf{transformation} & ODHQL Statement, welches die Transformation definiert. Falls die referenzierten Datenquellen nicht existieren wird ein Template erstellt. \\
}{Parameter für Erstellung einer Transformation}{tab:pd:api-transformation-create}

Hinweis: Vor der Erstellung einer Transformation sollte geprüft werden, ob das ODHQL Statement korrekt ist. Siehe dazu \cref{sec:pd:api-parse}.

\begin{srclst}{json}{Beispielanfrage für POST /api/v1/transformation/}
{
  "name":"Beispiel",
  "description":"Dies ist ein Beispiel für die Dokumentation.",
  "transformation":"SELECT t1.children,\nt1.city,\nt1.country,\nt1.email \nFROM \"ODH2_mockaroo.com\" as t1 \n",
  "private":true
}
\end{srclst}

\subsubsection{Transformations-Update - PUT /api/v1/transformation/<id>/}
Aktualisiert eine Transformation. Es können alle in \cref{tab:pd:api-transformation-create} erwähnten Felder aktualisiert werden.

\subsubsection{Preview für AdHoc-Transformationen - POST /api/v1/transformation/adhoc/}
Erstellt eine Preview für ein ODHQL Statement, ohne die Transformation in der Datenbank zu persistieren. Dies ist nützlich um eine Vorschau realisieren zu können, während der Benutzer noch am Bearbeiten seines Statements ist.

\subsection{File Groups}\label{sec:pd:api-filegroups}
Von Benutzern bereitgestellte Daten werden in sog. File Groups organisiert. Dies sind zusammengehörende Dateien, z.B. die .shx-, .shp- und .dbf-Datei bei einem ESRI Shapefile.

\subsubsection{File Group-Liste - GET /api/v1/fileGroups/}
Liefert eine Liste aller vorhandene File Groups.

\begin{srclst}{json}{Beispielantwort für GET /api/v1/fileGroups/}
[
    {
        "id": 1, 
        "url": "http://beta.opendatahub.ch/api/v1/fileGroup/1/", 
        "document": {
            "id": 1, 
            "url": "http://beta.opendatahub.ch/api/v1/document/1/", 
            "name": "Test mockaroo.com.csv", 
            "description": "Testdaten (Originalformat: CSV)", 
            "file_groups": "http://beta.opendatahub.ch/api/v1/document/1/filegroup/", 
            "private": false, 
            "owner": {
                "id": 1, 
                "username": "testuser", 
                "first_name": "", 
                "last_name": ""
            }, 
            "created_at": "2015-06-01T12:35:31.480135Z", 
            "preview": "http://beta.opendatahub.ch/api/v1/document/1/preview/", 
            "type": "document"
        }, 
        "files": [
            {
                "id": 1, 
                "url": "http://beta.opendatahub.ch/api/v1/file/1/", 
                "file_name": "mockaroo.com.csv", 
                "file_format": "CSV", 
                "file_group": "http://beta.opendatahub.ch/api/v1/fileGroup/1/"
            }
        ], 
        "urls": [], 
        "data": "http://beta.opendatahub.ch/api/v1/fileGroup/1/data/", 
        "preview": "http://beta.opendatahub.ch/api/v1/fileGroup/1/preview/"
    }, 
    ...
]
\end{srclst}

Siehe auch \cref{sec:pd:api-filegroup-detail}.

\subsubsection{File Group-Details - GET /api/v1/fileGroup/<id>/} \label{sec:pd:api-filegroup-detail}
Liefert Detail-Informationen zu einer File Group.

\begin{srclst}{json}{Beispielantwort für GET /api/v1/fileGroup/3/}
{
    "id": 3, 
    "url": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/fileGroup/3/", 
    "document": {
        "id": 3, 
        "url": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/document/3/", 
        "name": "Test mockaroo.com.xlsx", 
        "description": "Testdaten (Originalformat: Excel)", 
        "file_groups": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/document/3/filegroup/", 
        "private": false, 
        "owner": {
            "id": 1, 
            "username": "testuser", 
            "first_name": "", 
            "last_name": ""
        }, 
        "created_at": "2015-06-02T12:01:21.730842Z", 
        "preview": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/document/3/preview/", 
        "type": "document"
    }, 
    "files": [
        {
            "id": 3, 
            "url": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/file/3/", 
            "file_name": "mockaroo.com.xlsx", 
            "file_format": "Excel", 
            "file_group": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/fileGroup/3/"
        }
    ], 
    "urls": [], 
    "data": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/fileGroup/3/data/", 
    "preview": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/fileGroup/3/preview/"
}
\end{srclst}

\paragraph{Document} Das Dokument, zu dem diese File Group gehört. Siehe auch \cref{sec:pd:api-document-details} bzw. \cref{sec:pd:api-package-details}.
\paragraph{Files} Führt die zur File Group gehörenden Dateien auf. Siehe auch \cref{sec:pd:api-files}.
\paragraph{Urls} Führt die zur File Group gehörenden Urls auf. Siehe auch \cref{sec:pd:api-urls}.
\paragraph{Data} Url für den Download der Daten. Siehe auch \cref{sec:pd:api-download}.
\paragraph{Preview} Url für eine Vorschau der Daten in dieser File Group. Siehe auch \cref{sec:pd:api-preview}.

\subsection{Files} \label{sec:pd:api-files}
\subsubsection{File-Liste - GET /api/v1/files/}
Liefert eine Liste der vorhandenen Dateien.

\begin{srclst}{json}{Beispielantwort für GET /api/v1/file/}
{
    "count": 41, 
    "next": "http://localhost:5000/api/v1/file/?page=2", 
    "previous": null, 
    "results": [
        {
          "id": 1, 
          "url": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/file/1/", 
          "file_name": "mockaroo.com.csv", 
          "file_format": "CSV", 
          "file_group": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/fileGroup/1/"
        }, 
        ...
    ]
}
\end{srclst}

Dieser Endpunkt unterstützt Paging (\cref{sec:pd:api-paging}).

\subsubsection{File-Details - GET /api/v1/file/<id>}
Liefert Informationen zu einer einzelnen Datei.

\begin{srclst}{json}{Beispielantwort für GET /api/v1/file/1/}
{
    "id": 1, 
    "url": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/file/1/", 
    "file_name": "mockaroo.com.csv", 
    "file_format": "CSV", 
    "file_group": "https://opendatahub-hsr-dev.herokuapp.com/api/v1/fileGroup/1/"
}
\end{srclst}

\subsection{Urls} \label{sec:pd:api-urls}
\subsubsection{Url-Liste - GET /api/v1/url/}
Liefert eine Liste der vorhandenen Urls.

\begin{srclst}{json}{Beispielantwort für GET /api/v1/url/}
{
    "count": 2, 
    "next": null, 
    "previous": null, 
    "results": [
        {
            "id": 1, 
            "url": "http://localhost:5000/api/v1/url/1/", 
            "source_url": "http://maps.zh.ch/wfs/HaltestellenZHWFS", 
            "url_format": null, 
            "refresh_after": 3600, 
            "type": "wfs", 
            "file_group": "http://localhost:5000/api/v1/fileGroup/1001/"
        }, 
        ...
    ]
}
\end{srclst}

Dieser Endpunkt unterstützt Paging (\cref{sec:pd:api-paging}).

\subsubsection{Url-Liste - GET /api/v1/url/1/}
Liefert Detail-Informationen zur Url.

\begin{srclst}{json}{Beispielantwort für GET /api/v1/url/1/}
{
    "id": 1, 
    "url": "http://localhost:5000/api/v1/url/1/", 
    "source_url": "http://maps.zh.ch/wfs/HaltestellenZHWFS", 
    "url_format": null, 
    "refresh_after": 3600, 
    "type": "wfs", 
    "file_group": "http://localhost:5000/api/v1/fileGroup/1001/"
}
\end{srclst}

\subsection{Hilfs-Views}
Diese Views sind kein direkter Bestandteil des \gls{rest} \acs{api}s, dienen jedoch als Unterstützung dazu.

\subsubsection{ODHQL Checker - POST /api/v1/parse} \label{sec:pd:api-parse}
Parsed eine ODHQL Abfrage und liefert die Fehlermeldungen in einem zur Fehlersuche hilfreichen Format.

\subsubsection{Benutzerdokumentation - GET /api/v1/odhql/doc/}
Erstellt die Benutzerdokumentation anhand von Inline-Dokumentation im Code und liefert das Resultat als HTML-Seite zurück.

\section{User Interface}
\subsection{Authentifizierung}
Zur Authentifizierung der Benutzer wird der oAuth 2 Standard verwendet. Dieser erlaubt ein Login mit einem GitHub, FaceBook oder anderem Konto. Implementiert wurde dieser Standard auf der REST Seite mit Hilfe von verschiedenen Django Plugins. 
Beispielsweise wurde python-social-auth verwendet um den Benutzer gegenüber diesen Diensten zu authentisieren. Die REST Schnittstelle wird dann über einen Token gesichert.

\subsection{Transformationen erstellen}
Im Grundsatz wollten wir das Erstellen einer Transformation so einfach wie möglich gestalten. Zu diesem Zweck haben wir uns früh dazu entschlossen, einen Wizard zu schreiben, welcher die Aktion vereinfachen soll. In den frühen Phasen des Projektes war es notwendig den Wizard zu verwenden. In der Beta, die ausgeliefert wurde kann der Wizard verwendet werden, muss aber dank der Möglichkeit Tabellen ``zu laden'' nicht.
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{fig/odhql_wizard_early.png}
\caption{Tabellen können per Klick verbunden werden }
\label{fig:pd:tables_per_click}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{fig/wizard-step-one.png}
\caption{Einleitung in den Wizard}
\label{fig:pd:wizard-step-one}
\end{figure}
In einem ersten Schritt kann eine Transformation grundlegend erstellt werden. Einfache JOIN und UNION Operationen werden vom Benutzer konfiguriert.
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{fig/wizard-manual.png}
\caption{Transformation manuell bearbeiten}
\label{fig:pd:wizard-manual}
\end{figure}

Später können dann die komplexeren Operationen auf die Daten angewendet werden. Hierfür wird der manuelle Modus verwendet. Im manuellen Modus steht eine Hilfe Seite zur Verfügung, welche sämtliche erweiterten Operationen erklärt.\\
Wird eine Transformation ``als Template'' verwendet, wird beim Speichern keine Prüfung der zugrunde legenden Daten durchgeführt. So können Syntaktisch korrekte, aber noch mit fehlenden Daten belastete Transformationen erstellt werden.

\subsection{Transformationen bearbeiten}
Transformationen können entweder geklont oder überschrieben werden. Beim Klonen legt der Benutzer eine neue Transformation auf Grund der Daten einer Bestehenden Transformation an. Beim Überschreiben kann der Besitzer einer Transformation diese verändern. Es kann bei diesen Operationen allerdings kein Wizard mehr verwendet werden, da dieser alle manuellen Änderungen überschreiben würde.
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{fig/transformation-edit.png}
\caption{Bestehende Transformation editieren}
\label{fig:pd:transformation-manual-edit}
\end{figure}

\chapter{Testing}
\section{User Interface}

Bei allen folgenden Tests wird angenommen, dass der Benutzer die Web-Applikation bereits geladen hat. Für die Tests wird entweder \url{https://dev.opendatahub.ch} oder eine lokale Instanz (erreichbar unter \url{http://localhost:5000/}) verwendet.

Als Testpersonen stellten sich zur Verfügung:
\begin{enumerate}
\item Philipp Christen
\end{enumerate}

\subsection{UI01: Anmelden}
\paragraph{Anmeldestatus} Nicht angemeldet.

\paragraph{Aufgabe} Der Benutzer will sich anmelden.

\paragraph{Walkthrough}
\begin{enumerate}
\item Der Benutzer klickt auf ``Anmelden/Registrieren''.
\item Der Benutzer meldet sich mit einem der angebotenen OAuth-Providern an.
\item Ein Pop-Up mit dem normalen Login UI des Providers erscheint. Der Benutzer meldet sich an.
  \begin{enumerate}[label=\labelenumi\alph*.]
  \item Die Anmeldung beim Provider schlägt fehl. Der Anmelde-Vorgang wird abgebrochen.
  \end{enumerate}
\item Nach kurzer Wartezeit ist der Benutzer bei OpenDataHub angemeldet.
\end{enumerate}

\paragraph{Testperson 1}
\begin{enumerate}
\item Der Benutzer sieht den ``Anmelden''-Button und klickt darauf.
\item Da er einen Github-Account hat und dort bereits angemeldet ist, wählt er diesen Provider.
\item Die Anmeldung erfolgt problemlos. Der Benutzer wünscht sich anschliessend zu sehen, wer eingeloggt ist (Name fehlt).
\end{enumerate}

\subsection{UI02: Daten bereitstellen}
\paragraph{Anmeldestatus} Angemeldet.

\paragraph{Aufgabe} Der Benutzer will die Daten aus der Datei ``Baustellen Mai 2015.xls'' mit einer Liste von Baustellen im Appenzell Ausserrhoden anbieten.

\paragraph{Walkthrough}
\begin{enumerate}
\item Der Benutzer klickt auf ``Teilen''.
\item Der Benutzer füllt die Felder ``Name/Titel'' sowie ``Beschreibung'' aus. \\
      Zum Beispiel: Name: ``Baustellen Mai 2015'', Beschreibung: ``Baustellen-Liste Appenzell Ausserrhoden Mai 2015''.
\item Die Felder ``Privat'' sowie ``Format'' werden ignoriert.
  \begin{enumerate}[label=\labelenumi\alph*.]
  \item Die Daten sollen privat bleiben. Der Benutzer klickt das Feld ``Privat'' an.
  \item Der Benutzer will sicherstellen, dass das Format richtig erkannt wird und wählt aus der Liste das Format ``Excel'' aus.
  \end{enumerate}
\item Da die Daten als Datei vorliegen wählt der Benutzer ``Dateien hochladen'' aus.
\item Anschliessend zieht er die Datei von einem File-Browser direkt auf das Upload-Feld.
\item Ein Klick auf den Button ``Teilen'' lädt die Datei hoch. Eine grün hinterlegte Nachricht ``Ihre Daten wurden gespeichert'' erscheint.  
  \begin{enumerate}[label=\labelenumi\alph*.]
  \item Die Daten liegen in einem Format vor, welches von OpenDataHub nicht erkannt wird. Es wird eine rot hinterlegte Fehlermeldung angezeigt.
  \end{enumerate}
\item Nach kurzer Zeit wird der Benutzer auf die Detail-Ansicht seiner Daten weitergeleitet und sieht eine Vorschau seiner Daten.
\end{enumerate}

\paragraph{Testperson 1}
\begin{enumerate}
\item Der Benutzer interpretiert ``Anbieten'' korrekt als ``Teilen'' und klickt den entsprechenden Menu-Eintrag an.
\item Als Format belässt er ``Auto'', da er ``faul'' sei.
\item Bei der Datei-Auswahl bemängelt der Benutzer, dass der Datei-Name abgeschnitten wird, obwohl noch genug Platz vorhanden ist.
\item Beim Speichern freut er sich über die Toast-Message (``grün!''), liest den Text jedoch nicht durch.
\item Nach erfolgreichem Speichern ist der Benutzer unsicher, ob die Aufgabe abgeschlossen ist und sucht nach Share-/Social-Buttons.
\end{enumerate}

\subsection{UI03: Daten transformieren (Mit Assistent)}\label{sec:ui-test-trf-ass}
\paragraph{Anmeldestatus} Angemeldet.
\paragraph{Aufgabe} Der Benutzer interessiert sich für Baustellen auf Zürcher Kantonsstrassen. Er will Gemeinde, Baustellen-Status, Beginn und Ende der Bauarbeiten sowie den Strassennamen, jeweils mit Menschen-lesbaren Feld-Bezeichnungen, und erstellt dazu eine Transformation.

Der Benutzer hat keine Erfahrungen mit ODHQL oder SQL und verwendet den Assistenten.

\paragraph{Walkthrough}
\begin{enumerate}
\item Der Benutzer klickt auf ``Neue Transformation''.
\item In typischer Benutzer-Manier wird die Anleitung auf der Einstiegs-Seite ignoriert.
\item Der Benutzer klickt auf ``Assistent''.
\item \label{ui-test-assist-begin} Die gewünschten Daten sind nicht auf der ersten Seite zu finden. Daher verwendet der Benutzer das Suchfeld.
  \begin{enumerate}[label=\labelenumi\alph*.]
  \item Der Benutzer übersieht das Suchfeld und klickt durch die Seiten bis er die gewünschten Daten findet.
  \end{enumerate}
\item Nach erfolgreicher Suche klickt der Benutzer auf den Namen des Packages (Dokument/Transformation).
\item Ein Klick auf den Tabellen-Namen ``ODH18\_baustellen\_detailansicht'' fügt die Tabelle zur Abfrage hinzu.
\item Der Benutzer wählt die in der Aufgabe erwähnten Felder durch einen Klick auf ``Feld hinzufügen'' aus.
\item \label{ui-test-assist-end}Durch bearbeiten der Text-Felder in den Spalten-Titeln benennt der Benutzer die Felder um.
\item Anschliessend vergibt er einen Namen sowie eine Beschreibung und klickt auf den ``Speichern''-Button.
\item Eine grüne Erfolgsmeldung erscheint und nach kurzer Zeit wird der Benutzer zur Detail-Ansicht der neuen Transformation weitergeleitet.
\end{enumerate}

\paragraph{Testperson 1}
\begin{enumerate}
\item Nach kurzer Orientierung in der Benutzeroberfläche findet der Benutzer das Suchfeld.
\item Die Suche nach ``baustelle'' ist erfolgreich. Der Benutzer ist unsicher, ob ``baustellen\_detailansicht'' oder ``baustellen\_uebersicht'' richtig ist und wird angewiesen, die ``Detail''-Version auszuwählen. Dies ist eine Unschönheit im WFS von dem die Daten stammen und kein UI-Problem.
\item Er ist unsicher, wo er klicken muss um die Felder hinzuzufügen. Nach kurzer Orientierung findet er die richtige Stelle (alles ausser Alias-Feld).
\item Bei der Eingabe der Aliases ist der Benutzer enttäuscht, dass die Tabelle nicht automatisch weiterscrollt. 
\item Das Speichern der Tranformation funktioniert problemlos.
\end{enumerate}

\subsection{UI04: Daten transformieren (Ohne Assistent)}
\paragraph{Anmeldestatus} Angemeldet.

\paragraph{Aufgabe} Der Benutzer interessiert sich für Baustellen auf Zürcher Kantonsstrassen. Er will Gemeinde, Baustellen-Status, Beginn und Ende der Bauarbeiten, den Strassennamen sowie die Geometrie als Punkt, jeweils mit Menschen-lesbaren Namen, und erstellt dazu eine Transformation.

Das Datums-Format soll dem Format ``Tag. Monat. Jahr'' (z.B. ``01.01.1970'') entsprechen.

Da er bereits Erfahrung mit ODHQL oder SQL hat schreibt er die Transformation selbst. Der Assistent kann als Startpunkt verwendet werden, unterstützt jedoch nicht alle benötigte Funktionalität.

Hinweis: Die Sprache ODHQL lehnt sich stark an SQL an. Ausserdem ist in der Hilfe eine ODHQL-Referenz enthalten.

\paragraph{Walkthrough}
\begin{enumerate}
\item Der Benutzer klickt auf ``Neue Transformation'' und liest sich den Informations-Text durch.
\item Um die Liste der Spalten zu erhalten verwendet er den Assistenten (siehe \cref{sec:ui-test-trf-ass}, Schritte \crefrange{ui-test-assist-begin}{ui-test-assist-end}).
\item Durch Klick auf ``Manuelles Bearbeiten'' wechselt der Benutzer in den Editier-Modus.
\item Mit Hilfe der ODHQL-Referenz ergänzt der Benutzer die fehlenden Funktionen (ST\_Centroid, To\_Date, To\_Char). Zur Überprüfung der Abfrage verwendet der Benutzer periodisch den ``Vorschau''-Knopf.
\item Anschliessend vergibt er einen Namen sowie eine Beschreibung und klickt auf den ``Speichern''-Button.
\item Eine grüne Erfolgsmeldung erscheint und nach kurzer Zeit wird der Benutzer zur Detail-Ansicht der neuen Transformation weitergeleitet.
  \begin{enumerate}[label=\labelenumi\alph*.]
  \item Die Transformation ist fehlerhaft. Éine rote Fehlermeldung zeigt detailliertere Informationen zum Problem an.
  \end{enumerate}
\end{enumerate}

\paragraph{Testperson 1}
\begin{enumerate}
\item Der Benutzer verwendet den Assistenten als Startpunkt und will anschliessend in den Editier-Modus wechseln. Dazu muss er erst nach oben scrollen, was er als ``mühsam'' bezeichnet.
\item Das Hilfe-Icon funktioniert nicht. Die Punkt ``Hilfe'' im Menu schon, öffnet die Hilfe aber im selben Browser-Fenster. Der Benutzer muss die Abfrage nochmals neu zusammenklicken.
\item Nach der Eingabe eines fehlerhaften Queries stellt der Benutzer fest, dass die Fehlermeldung ungünstig platziert ist (unterhalb der Vorschau).
\item Der Benutzer hat einige Schwierigkeiten mit der Erstellung des Queries. Er hat zwar die Vorlesung DB1 besucht, verwendet SQL aber sehr selten. Mit Hilfe der Dokumentation gelingt es schliesslich doch.
\end{enumerate}

\subsection{UI05: Daten beziehen}
\paragraph{Anmeldestatus} Nicht angemeldet.
\paragraph{Aufgabe} Um die Daten in seinem System weiterverwenden zu können will der Benutzer die Resultate der vorhin erstellten Transformationen als GML-Datei herunterladen.

\paragraph{Walkthrough}
\begin{enumerate}
\item Der Benutzer sucht die Transformation in der Daten-Liste und klickt auf den Namen.
\item Ein Klick auf ``Herunterladen'' öffnet die Format-Auswahl.
\item Die Auswahl von ``GML'' führt zu einem erfolgreichen Download der Daten.
\end{enumerate}

\paragraph{Testperson 1}
Die Erfüllung der Aufgabe stellte kein Problem dar.

Nach Abschluss des Tests experimentierte der Benutzer ein wenig mit exotischeren Unicode-Namen. Namen komplett ohne Zeichen im ASCII-Bereich stellten ein Problem dar - der Download schlägt in diesen Fällen fehl. 
\chapter{Resultate und Ausblick}

\section{Resultate}
Die Resultate der Arbeit sind in \vref{sec:tb:results} beschrieben.

\section{Weiterentwicklung}

\subsection{GDAL 2.0}
Diverse Treiber wurden in GDAL 2.0 massiv verbessert (Interlis 1/2) bzw. richtig implementiert (GeoPackage). Sobald ein Release vorhanden ist sollte ein Upgrade in Betracht gezogen werden.

\subsection{Native Unterstützung für via ogr2ogr implementierte Formate}
Die Unterstützung für die Formate Interlis 1, GML und WFS (nur Parser) ist via ogr2ogr implementiert. Da ogr2ogr auf Datei-Ebene operiert müssen die Daten über ein Zwischenformat konvertiert werden.

Für den Formatter sieht das wie folgt aus:
$$ \text{ODH} \to \text{Zwischenformat} \to \text{ogr2ogr} \to \text{Zielformat} $$

Analog für den Parser. Beide Konvertierungs-Schritte sind potentiell fehlerbehaftet, ausserdem sind nicht alle ogr2ogr-Treiber von gleicher Qualität.

Daher wäre es wünschenswert, diese Formate mit nativen Parsern und Formattern zu unterstützen. 

\subsection{Unterstützung für Interlis 2}
Interlis 2 ist eine Weiterentwicklung von Interlis 1. Die uns zur Verfügung stehenden GDAL-Versionen hatten jedoch nur minimale Unterstützung für dieses Format.

\subsection{Ausbau der Webapplikation}
Nach Untersuchung der bereits vorhandenen Produkte wurde entschieden, die Webapplikation minimal zu halten. Die Konvertierung und Transformation von Daten hatte Priorität.

Wir sehen folgende Optionen:
\begin{itemize}
\item Ausbau der verwalteten Metadaten
\item Veränderung der Files, die zu einem Package gehören
\item Diskussion von Daten/Transformationen, evtl. Issue-Tracker
\item Integration von CKAN
\end{itemize}

\subsection{Mehr ODHQL-Funktionen}
Bisher sind zwar diverse Funktionen implementiert, gerade im Geo-Bereich gibt es jedoch noch einiges an Erweiterungspotential.

\subsection{Datentypen}
ODHQL unterstützt bisher folgende Datentypen: Integer (32 Bit), SmallInt (16 Bit), BigInt (64 Bit), Float, DateTime, Boolean, Text und Geometry. (Date-/Time-)Interval wird theoretisch unterstützt, es gibt jedoch aktuell keine Funktionen welche diesen Typ verwenden.

Mögliche Erweiterungen:
\begin{itemize}
\item Wertebereiche für bereits bestehende Datentypen, z.B. Längenbeschränkung für Text, oder Boundaries für Geometrien
\item Subqueries bzw. Nested Tables
\item Baumstrukturen, Key-Value-Pairs
\end{itemize}

Ausserdem sollte Untersucht werden, ob Geodaten in allen Fällen korrekt behandelt werden.

\subsection{Mehr Formate}
Nach Bedarf können weitere Formate implementiert werden.

\subsection{ODH als WFS-Server}
ODH unterstützt WFS als Daten-Quelle. Die Idee ist, dass eine API implementiert wird, welche eine Datenquelle oder WFS-Url und optional eine ODHQL-Abfrage entgegen nimmt, die Transformation durchführt und anschliessend das Resultat als WFS anbietet.

\subsection{Performanz bei grossen Datenmengen}
Im Rahmen der Arbeit wurden Dokumente mit Grössen bis ca. 100 MB (ca. 1 Mio Records) mit guten Resultaten getestet. Im Gespräch mit Herrn Dorfschmid wurde festgestellt, dass dies für viele Anwendungen ausreicht.

Falls massiv grössere Datenmengen verarbeitet werden müssen ist die aktuelle Architektur sub-optimal und limitiert durch den verfügbaren Speicher. In diesem Falle sollte evtl. auf eine Stream-basierte Version gewechselt werden.

Zu beachten ist, dass diverse verwendete Libraries (unter anderem auch GDAL) nicht Stream-orientiert arbeiten.

\subsection{Unterstützung für weitere Transformations-Arten}
Ogr2ogr ermöglicht es, Transformationen per SQL in einer Datenbank auszuführen. Dies könnte auch in ODH implementiert werden, würde jedoch zu Performance-Verlusten führen.

\subsection{Bereinigung des REST API}
Das \gls{rest} \acs{api} funktioniert, beinhaltet jedoch einige Inkonsistenzen bzw. Unschönheiten. 

Ein Beispiel:
Bei Anfragen auf \url{/api/v1/fileGroups/} wird das zur File Group gehörende Dokument zurückgeliefert.
\url{/api/v1/document/<id>/fileGroups/} liefert alle zu einem Dokument gehörenden File Groups. Auch hier enthält die Antwort das Dokument, da der selbe Serializer verwendet wird. Dies führt jedoch dazu, dass bei Dokumenten mit mehr als einer File Group die Dokument-Informationen mehrfach in der Antwort enthalten sind.

Stattdessen sollte in dieser Situation nur die Dokument-Url geliefert werden.
\subsection{Templates in neuen Transformationen verwenden können}
Anstatt ein Template zuerst als Transformation abzuspeichern, könnte es Sinnvoll sein, ein Template direkt in einer neuen Transformation verwenden zu können.

\subsection{In Templates Spaltennamen überschreiben}
Im Moment können im OpenDataHub die Tabellennamen generalisiert werden. Wenn ein Template andere Spaltennamen als das referenzierte Dokument verwendet, können diese nicht automatisch ausgetauscht werden.

\subsection{Kategorien und Tags für Dokumente und Transformationen}
In einer weiteren Version könnten Dokumente und Transformationen mit Tags und Kategorien versehen werden. Dies würde die Übersichtlichkeit erhöhen.

\subsection{Benutzer-Gruppen mit unterschiedlichen Rechten}
Zu Beginn der Arbeit wurde zusammen mit dem Betreuer entschieden, dass die Benutzer-Verwaltung minimal gehalten werden soll. Insbesondere sollte kein Aufwand zur Administration der Benutzer anfallen.

In Zukunft könnte es wünschenswert sein, Benutzergruppen mit unterschiedlichen Rechten einzuführen, z.B. eine Unterscheidung zwischen normalen Nutzern und Administratoren.