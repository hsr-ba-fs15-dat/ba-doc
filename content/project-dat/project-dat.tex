\part{Project DAT}

\chapter{Einführung}

In der heutigen Gesellschaft wird der Austausch von Daten immer wichtiger. Vermehrt bieten werden grosse Datenmengen unterschiedlichster Art - aus Forschung, Regierung oder zivilen Kreisen - zur allgemeinen Verwendung angeboten. Nicht nur die Art der Daten ist jedoch vielfältig, sondern auch Datenformat oder Zugriffsart unterscheiden sich.

Das \gls{dat}-Projekt will die Integration verschiedener solcher Datenquellen vereinfachen.

\section{Was ist dat?} % https://github.com/maxogden/dat/blob/master/docs/what-is-dat.md

Das dat-Projekt hat folgende Ziele: 

\begin{itemize}
\item Daten sollen automatisch zwischen unterschiedlichen dat-Instanzen synchronisiert werden können. 
\item Unterstützung grosser Datenmengen (Milliarden von Datensätzen bzw. Speicherbedarf im TB-Bereich), evtl. mit häufigen Aktualisierungen
\item Unterstützung von Daten in Tabellenform oder unstrukturiert
\item Plugin-basierte Schnittstelle zu bestehenden Datenbanken/formaten
\item Unterstützung für automatisierte Workflows
\end{itemize}

% https://github.com/maxogden/dat/blob/master/docs/js-api.md#transformations
Für den Umgang mit verschiedenen Datenformaten können Transformationen definiert werden, welche entweder vor Schreib- oder nach Lese-Operationen ausgeführt werden.

\section{Architektur}
% diagramm, requirements, interfaces, ...

% http://dat-data.com/docs.html: "Q:If I have multiple different tables that I want to store, would I create a separate dat repo for each? Can a single dat repo store multiple tables? A: Each dat repo is only one table."

\xxx[Architektur-doc finden/erstellen]

\section{Use Cases} % wofür zum teufel ist das teil gedacht. einige use cass sind online beschrieben

\subsection{Astronomie: Trillian} 
% https://github.com/maxogden/dat/issues/172
% https://github.com/trillian/trillian
% http://trillianverse.com (nicht verfügbar 2015-02-23)
In der Astronomie fallen riesige Datenmengen an. Teilweise werden diese als grosse Daten-Releases zur Verfügung gestellt (z.B. Sloan Digital Sky Survey), welche frühere Releases komplett ersetzen. Andere Projekte stellen inkrementelle Updates zur Verfügung (z.B. Hubble). Viele Astronomie-Institute haben weder die Mittel noch das Know-How, mit solchen Datenmengen umgehen zu können. Das Trillian-Projekt kümmert sich um die Verwaltung dieser Daten und bietet eine Compute-Engine an, um neue Modelle anhand der vorhandenen Daten zu testen.

dat soll für den Import und die Indexierung der Daten verwendet werden.

\subsection{Biologie: Integration verschiedener Datenquellen} 
% https://github.com/maxogden/dat/issues/129
Unterschiedliche Datenbanken (USA: Gene Expression Omnibus, EU: ArrayExpress), unterschiedliche Datentypen. Anforderung: Integration der Daten von den verschiedenen Quellen, Query

\xxx[Ausformulieren]

\subsection{Regierung: Sammlung von Daten} % https://github.com/maxogden/dat/issues/153
Für Statistik- oder Regulierungszwecke sammeln Regierungen Daten von unterschiedlichsten Betrieben. Diese Daten müssen oft über ein mehr oder weniger brauchbares Portal abgeliefert werden.

Die Verwendung von dat bringt folgende Vorteile:
\begin{itemize}
\item Ermöglicht die Prüfung von Daten beim Import, bereits vor der Ablieferung der Daten
\item Ermöglicht Ergänzung oder Korrektur von bereits abgelieferten Daten.
\item Ersatz der bisher verwendeten, für jede Regierungsstelle neu kreierten und teuren, Portale durch eine standardisierte Lösung.
\end{itemize}

\chapter{Erweiterung}

\section{Datscript} %https://github.com/datproject/datscript
Auf der Website des dat-Projektes wird Datscript als Modul zur Erstellung von Daten-Pipelines erwähnt. Im verlinkten Git-Repository ist jedoch lediglich eine Serie von Beispielen zu finden, mit dem letzten Commit vom November 2014 (Stand 23. Februar 15).

In aktuelleren Diskussionen wird für Pipelines jeweils auf Gasket verwiesen.

\section{Gasket} % https://github.com/datproject/gasket
Gasket ist eine Node.js-basierte Pipeline-Engine. Die Konfiguration geschieht entweder im JSON-Format direkt in package.json, oder aber in einem Node.js-Modul. Unterstützt wird die Ausführung von externen Programmen, oder die Verwendung von Node.js-Modulen.

\section{Schnittstellen / API}

\subsection{REST}
% https://github.com/maxogden/dat/blob/master/docs/rest-api.md

Der dat-Server kann von jeder dat-Instanz durch \mintinline{console}{$ dat listen} gestartet werden.

dat implementiert aktuell nur HTTP Basic Authentication mit einem einzelnen Benutzer, welcher in dat.json konfiguriert werden kann. 

Falls ein Admin-Benutzer konfiguriert ist, können nur angemeldete Sessions Daten ändern, es können jedoch immer noch alle Daten von allen Benutzern gelesen werden.
% https://github.com/maxogden/dat/blob/master/docs/dat-json-config.md#adminuser-adminpass

SSL wird bisher nicht unterstützt.

\paragraph{GET /}
Liefert die dat-editor Webapplikation aus.

\paragraph{GET /api} Liefert allgemeine Informationen zur dat-Instanz
\begin{srclst}{json}{GET /api}
    {
        "dat":"Hello",
        "version":"6.9.6",
        "changes":2,
        "name":"dat-test",
        "rows":53,
        "approximateSize":{
            "rows":"3 kB"
        }
    }
\end{srclst}

\paragraph{GET /api/rows} 
Liefert eine Liste von Einträgen (Default: 50 Einträge).
% http://www.fakenamegenerator.com/gen-random-gr-sz.php ftw
\begin{srclst}{json}{GET /api/rows}
{
    "rows": [
        {
            "prename":"Leonie", "surname":"Hahn", 
            "street":"Via Camischolas sura", "nr":"86", 
            "zip":"1135", "city":"Denens", 
            "key":"ci6huzf52000057coxym5lgy7", "version":1
        },
        {
            "prename":"Manuela", "surname":"Eichel", 
            "street":"Vallerstrasse", "nr":"115",
            "zip":"3765", "city":"Pfaffenried", 
            "key":"ci6hv0d0a00005vco99jxr7w0", "version":1
        },
        //...
    ]
}
\end{srclst}

\paragraph{GET /api/rows/:key}
Liefert den Eintrag mit dem angegebenen Schlüssel, oder eine Fehlermeldung.

\begin{srclst}{json}{GET /api/rows/ci6huzf52000057coxym5lgy7}
{
    "prename":"Leonie", "surname":"Hahn", 
    "street":"Via Camischolas sura", "nr":"86", 
    "zip":"1135", "city":"Denens", 
    "key":"ci6huzf52000057coxym5lgy7", "version":1
}
\end{srclst}

\paragraph{POST /api/rows}
Fügt einen neuen Datensatz hinzu. Die Daten müssen als JSON vorliegen.

Die Antwort besteht entweder aus einer Konflikt-Meldung, oder aus dem neu hinzu gefügten Datensatz.

\begin{srclst}{console}{POST /api/rows}
$ curl -H 'Content-Type: application/json' -d '{"prename":"Markus", "surname":"Maier", "street":"Dreibündenstrasse", "nr":"135", "zip":"6774", "city":"Dalpe"}' -X POST localhost:6461/api/rows
{"prename":"Markus", "surname":"Maier", "street":"Dreibündenstrasse", "nr":"135", "zip":"6774", "city":"Dalpe", "key":"ci6hw717u0004x0cocfquv0j9", "version":1}
\end{srclst}

\paragraph{GET /api/rows/:key/:filename}
Liefert das Blob mit dem entsprechenden Filenamen, oder eine Fehlermeldung falls nicht verfügbar.

\paragraph{POST /api/rows/:key/:filename}
Fügt ein neues Blob zum angegebenen Datensatz hinzu. Der Datensatz muss bereits existieren, und die aktuelle Version des Datensatzes muss im Query-String angegeben werden: \mintinline{http}{POST /api/rows/foo/photo.jpg?version=1 HTTP/1.1}

\paragraph{GET /api/session}
Liefert Informationen zur aktuellen Session. Dieser Aufruf kann auch zur Anmeldung per Basic Authentication verwendet werden.

\paragraph{GET /api/login}
Selbe Funktionalität wie /api/session, setzt jedoch den HTTP-Header ``Basic realm="Secure Area"'', so dass Browser ein Login-Fenster anzeigen.

\paragraph{GET /api/logout}
Zerstört die aktuelle Session und entfernt Client-Side Cookies.

\paragraph{GET /api/changes}
Liefert eine Json-formatierte Version des Change Streams (siehe auch JavaScript-API createChangesStream) \xxx[proper ref]

\paragraph{GET /api/csv}
Liefert eine CSV-Datei mit der letzten Version der Daten.

\paragraph{POST /api/bulk}
Ermöglicht das Einfügen von mehreren Datensätzen gleichzeitig. Unterstützt werden JSON (Content-Type \texttt{application/json}) und CSV (Content-Type \texttt{text/csv}).

Falls die Daten akzeptiert werden besteht die Antwort aus einer Liste von JSON-Objekten mit Key und Version für die neu eingefügten Datensätze, andernfalls aus einem HTTP-Fehler.

\begin{srclst}{json}{GET /api/metadata}
{
    "changes":15, 
    "liveBackup":false, 
    "columns":["prename","surname","street","nr","zip","city","version:"]
}
\end{srclst}

\paragraph{GET /api/manifest}
Liefert ein JSON-Objekt mit Informationen zum DB-Backend. Dies wird für RPC verwendet (siehe nächsten Befehl).

\paragraph{POST /api/rpc}
Ein Server-Endpunkt für multilevel (ein Node.js-Modul).

\paragraph{GET /api/metadata}
Liefert Informationen zum Schema. Diese Daten werden während der Replikation verwendet.

\paragraph{GET /api/replicater/receive und GET /api/replicator/send}
Wird für Replikation verwendet.

\subsection{JavaScript}
%https://github.com/maxogden/dat/blob/master/docs/js-api.md
dat ist ein Node.js-Modul.

\begin{srclst}{js}{Laden des dat-Moduls}
var dat = require('dat')
\end{srclst}

\paragraph{Konstruktor}

Erstellt eine neue oder öffnet eine bereits bestehende dat-Datenbank. Alle Parameter sind optional.

\begin{srclst}{js}{Konstruktor}
var db = dat([path], [options], [onReady])
\end{srclst}

\begin{description}
\item[path (string)] Pfad zum Verzeichnis, welches das \texttt{.dat}-Verzeichnis enthält. Falls kein \texttt{.dat}-Verzeichnis existiert wird ein neues erstellt. Default ist \mintinline{js}{process.cwd()}.
\item[options (object)] Weitere Konfigurations-Parameter:
    \begin{description}
    \item[init] Wenn \mintinline{js}{false} erstellt dat keine neue Datenbank beim Initialisieren. Default: \mintinline{js}{true}.
    \item[storage] Wenn \mintinline{js}{false} versucht dat nicht, die Datenbank zu lesen beim Initialisieren. Default: \mintinline{js}{true}.
    \item[path] Alternative zum Konstruktor-Argument.
    \item[adminUser, adminPass] Wird als Benutzer/Passwort verwendet fúr HTTP Basic Authentication wenn beide konfiguriert sind.
    \item[leveldown] Benutzerdefiniertes \texttt{leveldown}-Backend. Default: \mintinline{js}{require('leveldown-prebuilt')}
    \item[db] Benutzerdefinierte \texttt{levelup}-Instanz. Wenn diese Option vorhanden ist wird die \texttt{leveldown}-Option ignoriert und alle Tabellen-basierte Daten werden in dieser DB-Instanz gespeichert.
    \item[blobs] Benutzerdefinierte \texttt{blob}-Datenbank. Default: \mintinline{js}{require('lib/blobs.js')}
    \item[replicator] Benutzerdefinierte Replicator-Instanz. Default: \mintinline{js}{require('lib/replicator.js')}
    \item[remoteAddress] Falls angegeben started dat im RPC Client-Modus. Default: \mintinline{js}{undefined}
    \item[manifest] Wenn \texttt{remoteAddress} gesetzt ist wird dies als Manifest für RPC verwendet.
    \item[skim] Wenn \mintinline{js}{true} wird lazy auf Blobs von einer nicht-lokalen Quelle zugegriffen. Default: \mintinline{js}{false}.
    \item[transformations] siehe unten
    \item[hooks] siehe unten
    \end{description}
\item[onReady (err) $\to$ ()] Wird aufgerufen nachdem dat initialisiert wurde. Wenn \texttt{err} gesetzt ist trat ein Fehler auf.
\end{description}
\xxx[Syntax für Funktionsaufrufe festlegen]

\paragraph{Transformationen}
Transformationen können vor \texttt{put}- oder nach \texttt{get}-Operationen ausgeführt werden. Die Konfiguration erfolgt über den \texttt{options}-Parameter im Konstruktor.

\begin{srclst}{js}{Beispiel einer Transformations-Konfiguration}
{
  "transformations": {
    "get": "transform-uppercase",
    "put": [{"module": "./lowercase-stream.js"}]
  }
}
\end{srclst}

Folgende Datentypen sind möglich:
\begin{description}
\item[string] Ausführbarer Befehl, um die Daten zu transformieren. Der Befehl erhält durch Zeilenumbrüche getrennte JSON-Datensätze (NDJSON) in \texttt{STDIN}. Nach der Transformation werden die Daten wieder als JSON auf \texttt{STDOUT} erwartet.
\item[object] Objekt mit einem der folgenden Feldern:
    \begin{description}
    \item[command] Gleiche Funktionalität wie oben
    \item[module] Per \mintinline{js}{require()} ladbares Node.js-Modul. Das Modul muss einen Streams2-Passthrough Stream exportieren mit \mintinline{js}{objectMode: true}.
    \end{description}
\item[array] Array von Transformationen, welche nacheinander ausgeführt werden.
\end{description}

\subsection{CLI}
%https://github.com/maxogden/dat/blob/master/docs/cli-usage.md

\subsection{Python}
%https://github.com/pkafei/Dat-Python


Mit datPython\footnote{\url{https://github.com/pkafei/Dat-Python}} existiert eine sehr simple Python API welche folgende Grundlegende Operationen mittels \gls{rest} Schnittstelle von \gls{dat} ermöglicht.

\begin{description}
	\item[\path{info()}] Generelle Informationen über das \gls{dat} Repository.
	\item[\path{diff()}] Gibt alle Änderungen zurück.
	\item[\path{csv()}] Gibt die Daten aus dem dat Repository als \gls{csv} formatierte Zeichenkette zurück.
	\item[\path{rows()}] Gibt alle Zeilen im dat Repository zurück.
	\item[\path{dict()}] Dasselbe wie \path{rows()} jedoch als Dictionary.
	\item[\path{put_json()}] Importiert alle Datensätze einer JSON Datei.
	\item[\path{put_csv()}] Importiert alle Datensätze einer \gls{csv} Datei.
\end{description}

datPython ist zurzeit lediglich ein minimaler HTTP Wrapper und besteht aus knapp 60 Programmcodezeilen. Die Installation mittels \gls{pypi} bzw. \texttt{pip} wäre zwar möglich, jedoch nicht Funktionsfähig. Bei Verwendung dieses noch kleinen Moduls wäre die Erstellung eines eigenen Forks zum jetzigen Zeitpunkt die beste Lösung.

\begin{srclst}{pycon}{datPython Verwendung}
>>> from datPython import Dat
>>> dat = Dat('http://7hhtoqpk6c8wu3di.c.try-dat.com')

>>> dat.info()
{"dat":"Hello","version":"6.8.4","changes":2,"name":"root","rows":1,"approximateSize":{"rows":"502 B"}}

>>> dat.diff()
{"change":1,"key":"schema","from":0,"to":1,"subset":"internal"}
{"change":2,"key":"ci6i1errr000012t5m9ggeh6h","from":0,"to":1}

>>> dat.csv()
key,version,name,age
ci6i1errr000012t5m9ggeh6h,1,alice,35

>>> dat.rows()
{"rows": [
	{"name":"alice","age":"35","key":"ci6i1errr000012t5m9ggeh6h","version":1}
	]}

>>> dat.dict()
{u'rows': [{u'age': u'35', u'version': 1, u'name': u'alice', u'key': u'ci6i1errr000012t5m9ggeh6h'}]}

>>> dat.put_json('test.json')
>>> dat.put_csv('test.csv')
\end{srclst}

